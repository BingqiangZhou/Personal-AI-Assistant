"""
æ’­å®¢éŸ³é¢‘è½¬å½•æœåŠ¡

æä¾›éŸ³é¢‘ä¸‹è½½ã€æ ¼å¼è½¬æ¢ã€æ–‡ä»¶åˆ‡å‰²ã€APIè½¬å½•å’Œç»“æœåˆå¹¶çš„å®Œæ•´åŠŸèƒ½
"""

import asyncio
import hashlib
import logging
import os
import time
from dataclasses import dataclass
from datetime import datetime, timezone

import aiofiles
import aiohttp
import ffmpeg
from fastapi import HTTPException, status
from sqlalchemy import select, update
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.core.exceptions import ValidationError
from app.domains.ai.models import ModelType
from app.domains.ai.repositories import AIModelConfigRepository
from app.domains.podcast.models import (
    PodcastEpisode,
    TranscriptionStatus,
    TranscriptionStep,
    TranscriptionTask,
)
from app.domains.podcast.summary_manager import DatabaseBackedAISummaryService


logger = logging.getLogger(__name__)


def log_with_timestamp(level: str, message: str, task_id: int = None):
    """
    è¾“å‡ºå¸¦æ—¶é—´æˆ³çš„æ—¥å¿—

    Args:
        level: æ—¥å¿—çº§åˆ« (INFO, WARNING, ERROR, DEBUG)
        message: æ—¥å¿—æ¶ˆæ¯
        task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    task_info = f"[Task:{task_id}] " if task_id is not None else ""
    formatted_message = f"{timestamp} {task_info}{message}"

    if level == "INFO":
        logger.info(formatted_message)
    elif level == "WARNING":
        logger.warning(formatted_message)
    elif level == "ERROR":
        logger.error(formatted_message)
    elif level == "DEBUG":
        logger.debug(formatted_message)
    else:
        logger.info(formatted_message)


@dataclass
class AudioChunk:
    """éŸ³é¢‘åˆ†ç‰‡ä¿¡æ¯"""
    index: int
    file_path: str
    start_time: float  # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    duration: float  # æ—¶é•¿ï¼ˆç§’ï¼‰
    file_size: int  # æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    transcript: str | None = None  # è½¬å½•ç»“æœ


@dataclass
class TranscriptionProgress:
    """è½¬å½•è¿›åº¦ä¿¡æ¯"""
    task_id: int
    status: TranscriptionStatus
    progress: float  # 0-100
    message: str
    current_chunk: int = 0
    total_chunks: int = 0


class AudioDownloader:
    """éŸ³é¢‘æ–‡ä»¶ä¸‹è½½å™¨"""

    def __init__(self, timeout: int = 300, chunk_size: int = 8192):
        self.timeout = timeout
        self.chunk_size = chunk_size
        self.session: aiohttp.ClientSession | None = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(limit=10, limit_per_host=5)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        # ä½¿ç”¨å®Œæ•´çš„æµè§ˆå™¨å¤´éƒ¨ä»¥ç»•è¿‡ CDN é˜²æŠ¤ï¼ˆCloudflareç­‰ï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=headers
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    async def download_file(self, url: str, destination: str, progress_callback=None) -> tuple[str, int]:
        """
        ä¸‹è½½æ–‡ä»¶åˆ°æŒ‡å®šä½ç½®

        Args:
            url: ä¸‹è½½URL
            destination: ä¿å­˜è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            Tuple[str, int]: (æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å¤§å°)
        """
        if not self.session:
            raise RuntimeError("AudioDownloader must be used as async context manager")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(destination), exist_ok=True)

        # å¤„ç† lizhi.fm çš„ CDN URL
        original_url = url
        if 'cdn.lizhi.fm' in url:
            url = url.replace('cdn.lizhi.fm', 'cdn.gzlzfm.com')
            logger.info(f"ğŸ”„ [CDN REPLACEMENT] Replaced CDN URL: {original_url[:80]}... -> {url[:80]}...")

        # å‡†å¤‡è¯·æ±‚å¤´
        request_headers = dict(self.session.headers)
        # ä¸º lizhi.fm æ·»åŠ  Referer
        if 'lizhi.fm' in original_url or 'lizhi.fm' in url or 'gzlzfm.com' in url:
            request_headers['Referer'] = 'https://www.lizhi.fm/'
            logger.info("ğŸ“‹ [HEADERS] Added Referer for lizhi.fm: https://www.lizhi.fm/")

        # è¾“å‡ºè¯·æ±‚å¤´ä¿¡æ¯ç”¨äºè°ƒè¯•
        logger.info(f"ğŸ“¤ [HTTP REQUEST] URL: {url}")
        logger.info(f"ğŸ“¤ [HTTP REQUEST] Headers: {request_headers}")

        try:
            async with self.session.get(url, headers=request_headers) as response:
                # â„¹ï¸ è¾“å‡ºå“åº”å¤´ä¿¡æ¯
                logger.info(f"â„¹ï¸ [Response Headers] {dict(response.headers)}")

                if response.status != 200:
                    raise HTTPException(
                        status_code=status.HTTP_400_BAD_REQUEST,
                        detail=f"Failed to download audio file: HTTP {response.status}"
                    )

                # è·å–æ–‡ä»¶å¤§å°
                content_length = response.headers.get('content-length')
                total_size = int(content_length) if content_length else 0

                # ä¸‹è½½æ–‡ä»¶
                downloaded = 0
                first_chunk_logged = False
                async with aiofiles.open(destination, 'wb') as f:
                    async for chunk in response.content.iter_chunked(self.chunk_size):
                        # â„¹ï¸ è¾“å‡ºç¬¬ä¸€ä¸ªchunkçš„å‰200å­—èŠ‚
                        if not first_chunk_logged:
                            preview = chunk[:200]
                            logger.info(f"â„¹ï¸ [Response Body Preview] First 200 bytes: {preview}")
                            first_chunk_logged = True

                        await f.write(chunk)
                        downloaded += len(chunk)

                        # è°ƒç”¨è¿›åº¦å›è°ƒ
                        if progress_callback and total_size > 0:
                            progress = (downloaded / total_size) * 100
                            await progress_callback(progress)

                logger.info(f"Successfully downloaded file to {destination}, size: {downloaded} bytes")
                return destination, downloaded

        except asyncio.TimeoutError:
            raise HTTPException(
                status_code=status.HTTP_408_REQUEST_TIMEOUT,
                detail="Download timeout"
            )
        except Exception as e:
            logger.error(f"Download failed: {str(e)}")
            # æ¸…ç†éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
            if os.path.exists(destination):
                os.remove(destination)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Download failed: {str(e)}"
            )

    async def download_file_with_fallback(
        self,
        url: str,
        destination: str,
        progress_callback=None
    ) -> tuple[str, int]:
        """
        æ–‡ä»¶ä¸‹è½½ï¼ˆç›´æ¥ä½¿ç”¨ aiohttpï¼Œæ— å›é€€ï¼‰

        Args:
            url: ä¸‹è½½URL
            destination: ä¿å­˜è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            Tuple[str, int]: (æ–‡ä»¶è·¯å¾„, æ–‡ä»¶å¤§å°)

        Raises:
            HTTPException: å¦‚æœä¸‹è½½å¤±è´¥
        """
        # ç›´æ¥ä½¿ç”¨ aiohttp ä¸‹è½½
        logger.info(f"ğŸ“¥ [DOWNLOAD] Starting download for: {url[:100]}...")
        try:
            file_path, file_size = await self.download_file(url, destination, progress_callback)
            logger.info(f"âœ… [DOWNLOAD] Download succeeded: {file_size} bytes")
            return file_path, file_size

        except Exception as e:
            logger.error(f"âŒ [DOWNLOAD] Download failed: {type(e).__name__}: {str(e)}")
            if isinstance(e, HTTPException):
                raise
            else:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"Download failed: {str(e)}"
                )


# Note: Browser fallback download has been removed.
# The download now uses only aiohttp with proper headers and retry logic.


class AudioConverter:
    """éŸ³é¢‘æ ¼å¼è½¬æ¢å™¨"""

    @staticmethod
    async def convert_to_mp3(input_path: str, output_path: str, progress_callback=None) -> tuple[str, float]:
        """
        å°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºMP3æ ¼å¼

        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºMP3æ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            Tuple[str, float]: (è¾“å‡ºæ–‡ä»¶è·¯å¾„, è½¬æ¢è€—æ—¶)
        """
        start_time = time.time()

        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(input_path):
                raise FileNotFoundError(f"Input file not found: {input_path}")

            input_size = os.path.getsize(input_path)
            logger.info(f"ğŸ§ [CONVERT] Starting conversion: {input_path} ({input_size/1024/1024:.2f} MB) -> {output_path}")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            # æ„å»ºFFmpegå‘½ä»¤
            ffmpeg_proc = (
                ffmpeg
                .input(input_path)
                .output(
                    output_path,
                    acodec='mp3',
                    ac=1,  # å•å£°é“
                    ar='16000',  # 16kHzé‡‡æ ·ç‡
                    ab='64k',  # 64kbpsæ¯”ç‰¹ç‡
                    f='mp3'
                )
                .overwrite_output()
                .global_args('-loglevel', 'error')  # Changed from 'quiet' to 'error' for debugging
            )

            # æ‰§è¡Œè½¬æ¢
            if progress_callback:
                await progress_callback(0)

            # ä½¿ç”¨å­è¿›ç¨‹æ‰§è¡ŒFFmpeg
            cmd = ffmpeg_proc.compile()
            logger.debug(f"ğŸ§ [CONVERT] FFmpeg command: {' '.join(cmd)}")

            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                error_msg = stderr.decode('utf-8', errors='replace') if stderr else "Unknown FFmpeg error"
                logger.error(f"ğŸ§ [CONVERT] FFmpeg failed with return code {process.returncode}")
                logger.error(f"ğŸ§ [CONVERT] FFmpeg stderr: {error_msg}")
                raise RuntimeError(f"FFmpeg conversion failed (code {process.returncode}): {error_msg}")

            # Verify output file was created
            if not os.path.exists(output_path):
                raise RuntimeError(f"FFmpeg completed successfully but output file not found: {output_path}")

            output_size = os.path.getsize(output_path)
            if output_size == 0:
                os.remove(output_path)
                raise RuntimeError(f"FFmpeg created empty output file: {output_path}")

            if progress_callback:
                await progress_callback(100)

            duration = time.time() - start_time
            logger.info(f"âœ… [CONVERT] Successfully converted {input_path} to {output_path}")
            logger.info(f"âœ… [CONVERT] Input: {input_size/1024/1024:.2f} MB -> Output: {output_size/1024/1024:.2f} MB, Time: {duration:.2f}s")

            return output_path, duration

        except Exception as e:
            logger.error(f"âŒ [CONVERT] Audio conversion failed: {type(e).__name__}: {str(e)}")
            logger.error(f"âŒ [CONVERT] Input: {input_path} (exists: {os.path.exists(input_path)}), Output: {output_path} (exists: {os.path.exists(output_path)})")
            # æ¸…ç†è¾“å‡ºæ–‡ä»¶ï¼ˆä¿ç•™ç”¨äºè°ƒè¯•ï¼‰
            if os.path.exists(output_path):
                try:
                    os.remove(output_path)
                    logger.debug(f"ğŸ§¹ [CONVERT] Removed partial output file: {output_path}")
                except Exception as cleanup_error:
                    logger.warning(f"âš ï¸ [CONVERT] Failed to remove partial output: {cleanup_error}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Audio conversion failed: {str(e)}"
            )


class AudioSplitter:
    """éŸ³é¢‘æ–‡ä»¶åˆ‡å‰²å™¨"""

    @staticmethod
    async def split_mp3_by_duration(
        input_path: str,
        output_dir: str,
        chunk_duration_seconds: int = 300,
        progress_callback=None
    ) -> list[AudioChunk]:
        """
        å°†MP3æ–‡ä»¶æŒ‰æ—¶é—´é•¿åº¦åˆ‡å‰²æˆç‰‡æ®µï¼ˆæ¨èç”¨äºè½¬å½•ï¼‰

        Args:
            input_path: è¾“å…¥MP3æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            chunk_duration_seconds: æ¯ä¸ªç‰‡æ®µçš„æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            List[AudioChunk]: åˆ‡å‰²åçš„éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨
        """
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)

            # ä½¿ç”¨FFmpegè·å–éŸ³é¢‘æ—¶é•¿
            probe = ffmpeg.probe(input_path)
            duration = float(probe['streams'][0]['duration'])

            # è®¡ç®—éœ€è¦åˆ‡å‰²çš„æ®µæ•°
            num_chunks = max(1, int(duration // chunk_duration_seconds) + (1 if duration % chunk_duration_seconds > 0 else 0))
            actual_chunk_duration = duration / num_chunks

            chunks = []
            base_name = os.path.splitext(os.path.basename(input_path))[0]

            for i in range(num_chunks):
                start_time = i * chunk_duration_seconds
                # æœ€åä¸€æ®µçš„æ—¶é•¿å¯èƒ½ä¸åŒ
                end_time = min(start_time + chunk_duration_seconds, duration)
                segment_duration = end_time - start_time

                output_path = os.path.join(
                    output_dir,
                    f"{base_name}_chunk_{i+1:03d}.mp3"
                )

                # ä½¿ç”¨FFmpegåˆ‡å‰² - ä½¿ç”¨æ—¶é—´å‚æ•°è€Œéæ–‡ä»¶å¤§å°
                (
                    ffmpeg
                    .input(input_path, ss=start_time, t=segment_duration)
                    .output(
                        output_path,
                        acodec='mp3',
                        ac=1,  # å•å£°é“
                        ar='16000',  # 16kHzé‡‡æ ·ç‡
                        ab='64k'  # 64kbpsæ¯”ç‰¹ç‡
                    )
                    .overwrite_output()
                    .global_args('-loglevel', 'quiet')
                    .run()
                )

                # è·å–åˆ‡å‰²åçš„æ–‡ä»¶å¤§å°
                chunk_file_size = os.path.getsize(output_path)

                chunk = AudioChunk(
                    index=i + 1,
                    file_path=output_path,
                    start_time=start_time,
                    duration=segment_duration,
                    file_size=chunk_file_size
                )
                chunks.append(chunk)

                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress = ((i + 1) / num_chunks) * 100
                    await progress_callback(progress)

            logger.info(f"Successfully split {input_path} into {len(chunks)} chunks by time ({chunk_duration_seconds}s each)")
            return chunks

        except Exception as e:
            logger.error(f"Audio splitting by time failed: {str(e)}")
            # æ¸…ç†å·²åˆ›å»ºçš„æ–‡ä»¶
            for chunk in locals().get('chunks', []):
                if os.path.exists(chunk.file_path):
                    os.remove(chunk.file_path)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Audio splitting by time failed: {str(e)}"
            )

    @staticmethod
    async def split_mp3(
        input_path: str,
        output_dir: str,
        chunk_size_mb: int = 10,
        progress_callback=None
    ) -> list[AudioChunk]:
        """
        å°†MP3æ–‡ä»¶åˆ‡å‰²æˆæŒ‡å®šå¤§å°çš„ç‰‡æ®µ

        Args:
            input_path: è¾“å…¥MP3æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            chunk_size_mb: æ¯ä¸ªç‰‡æ®µçš„å¤§å°ï¼ˆMBï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            List[AudioChunk]: åˆ‡å‰²åçš„éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨
        """
        try:
            # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
            if not os.path.exists(input_path):
                raise FileNotFoundError(f"Input file not found: {input_path}")

            input_size = os.path.getsize(input_path)
            logger.info(f"ğŸ”ª [SPLIT] Starting split: {input_path} ({input_size/1024/1024:.2f} MB) into {chunk_size_mb}MB chunks")

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"ğŸ”ª [SPLIT] Output directory: {output_dir}")

            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_size = os.path.getsize(input_path)
            chunk_size_bytes = chunk_size_mb * 1024 * 1024

            # ä½¿ç”¨FFmpegè·å–éŸ³é¢‘æ—¶é•¿
            try:
                probe = ffmpeg.probe(input_path)
                duration = float(probe['streams'][0]['duration'])
                logger.info(f"ğŸ”ª [SPLIT] Input duration: {duration:.2f}s")
            except Exception as e:
                logger.error(f"ğŸ”ª [SPLIT] FFmpeg probe failed: {e}")
                raise RuntimeError(f"Failed to probe input file: {e}")

            # è®¡ç®—éœ€è¦åˆ‡å‰²çš„æ®µæ•°
            num_chunks = max(1, (file_size + chunk_size_bytes - 1) // chunk_size_bytes)
            chunk_duration = duration / num_chunks

            logger.info(f"ğŸ”ª [SPLIT] Will create {num_chunks} chunks, ~{chunk_duration:.2f}s each")

            chunks = []
            base_name = os.path.splitext(os.path.basename(input_path))[0]

            for i in range(num_chunks):
                start_time = i * chunk_duration
                output_path = os.path.join(
                    output_dir,
                    f"{base_name}_chunk_{i+1:03d}.mp3"
                )

                logger.debug(f"ğŸ”ª [SPLIT] Creating chunk {i+1}/{num_chunks}: {output_path} (start: {start_time:.2f}s, duration: {chunk_duration:.2f}s)")

                # ä½¿ç”¨FFmpegåˆ‡å‰² - æ•è·è¾“å‡ºç”¨äºè°ƒè¯•
                try:
                    # æ„å»ºFFmpegå‘½ä»¤
                    ffmpeg_cmd = (
                        ffmpeg
                        .input(input_path, ss=start_time, t=chunk_duration)
                        .output(output_path, c='copy')
                        .overwrite_output()
                        .global_args('-loglevel', 'error')  # Changed from 'quiet' to 'error'
                        .compile()
                    )

                    # ä½¿ç”¨å­è¿›ç¨‹æ‰§è¡Œä»¥æ•è·é”™è¯¯
                    process = await asyncio.create_subprocess_exec(
                        *ffmpeg_cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )

                    stdout, stderr = await process.communicate()

                    if process.returncode != 0:
                        error_msg = stderr.decode('utf-8', errors='replace') if stderr else "Unknown error"
                        raise RuntimeError(f"FFmpeg split failed (code {process.returncode}): {error_msg}")

                except Exception as e:
                    logger.error(f"ğŸ”ª [SPLIT] Failed to create chunk {i+1}: {e}")
                    raise

                # éªŒè¯è¾“å‡ºæ–‡ä»¶è¢«åˆ›å»º
                if not os.path.exists(output_path):
                    raise RuntimeError(f"FFmpeg completed but output file not created: {output_path}")

                chunk_file_size = os.path.getsize(output_path)
                if chunk_file_size == 0:
                    os.remove(output_path)
                    raise RuntimeError(f"FFmpeg created empty chunk: {output_path}")

                chunk = AudioChunk(
                    index=i + 1,
                    file_path=output_path,
                    start_time=start_time,
                    duration=chunk_duration,
                    file_size=chunk_file_size
                )
                chunks.append(chunk)

                logger.debug(f"ğŸ”ª [SPLIT] Created chunk {i+1}: {chunk_file_size/1024:.2f} KB")

                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress = ((i + 1) / num_chunks) * 100
                    await progress_callback(progress)

            total_output_size = sum(c.file_size for c in chunks)
            logger.info(f"âœ… [SPLIT] Successfully split {input_path} into {len(chunks)} chunks ({total_output_size/1024/1024:.2f} MB total)")
            return chunks

        except Exception as e:
            logger.error(f"âŒ [SPLIT] Audio splitting failed: {type(e).__name__}: {str(e)}")
            logger.error(f"âŒ [SPLIT] Input: {input_path} (exists: {os.path.exists(input_path)}), Output dir: {output_dir}")
            # æ¸…ç†å·²åˆ›å»ºçš„æ–‡ä»¶
            for chunk in locals().get('chunks', []):
                if os.path.exists(chunk.file_path):
                    try:
                        os.remove(chunk.file_path)
                        logger.debug(f"ğŸ§¹ [SPLIT] Removed partial chunk: {chunk.file_path}")
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸ [SPLIT] Failed to remove partial chunk: {cleanup_error}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Audio splitting failed: {str(e)}"
            )


class SiliconFlowTranscriber:
    """ç¡…åŸºæµåŠ¨APIè½¬å½•æœåŠ¡"""

    def __init__(self, api_key: str, api_url: str, max_concurrent: int = 4):
        self.api_key = api_key
        self.api_url = api_url
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session: aiohttp.ClientSession | None = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(limit=self.max_concurrent)
        timeout = aiohttp.ClientTimeout(total=600)  # 10åˆ†é’Ÿè¶…æ—¶

        # Debug logging for API configuration
        logger.info(f"ğŸ”‘ [API DEBUG] API URL: {self.api_url}")
        logger.info(f"ğŸ”‘ [API DEBUG] API Key (first 12 chars): {self.api_key[:12]}...")
        logger.info(f"ğŸ”‘ [API DEBUG] API Key (last 4 chars): ...{self.api_key[-4::]}")

        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'Authorization': f'Bearer {self.api_key}'}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    async def transcribe_chunk(
        self,
        chunk: AudioChunk,
        model: str = "FunAudioLLM/SenseVoiceSmall",
        ai_repo=None,
        config_db_id: int | None = None
    ) -> AudioChunk:
        """
        è½¬å½•å•ä¸ªéŸ³é¢‘ç‰‡æ®µ

        Args:
            chunk: éŸ³é¢‘ç‰‡æ®µ
            model: è½¬å½•æ¨¡å‹åç§°
            ai_repo: AIæ¨¡å‹é…ç½®ä»“åº“ï¼ˆç”¨äºè®°å½•ç»Ÿè®¡ï¼‰
            config_db_id: AIæ¨¡å‹é…ç½®æ•°æ®åº“ID

        Returns:
            AudioChunk: åŒ…å«è½¬å½•ç»“æœçš„éŸ³é¢‘ç‰‡æ®µ
        """
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            if not self.session:
                raise RuntimeError("Transcriber must be used as async context manager")

            max_retries = 3
            base_delay = 2  # seconds

            for attempt in range(max_retries):
                chunk_start = time.time()
                attempt_succeeded = False
                try:
                    logger.info(f"ğŸ¤ [CHUNK {chunk.index:03d}] Starting transcription (Attempt {attempt+1}/{max_retries}), file={os.path.basename(chunk.file_path)}, size={chunk.file_size} bytes, model={model}")

                    # å‡†å¤‡æ–‡ä»¶ä¸Šä¼  (Re-open file for each attempt)
                    data = aiohttp.FormData()
                    data.add_field('model', model)
                    data.add_field(
                        'file',
                        open(chunk.file_path, 'rb'),
                        filename=os.path.basename(chunk.file_path),
                        content_type='audio/mpeg'
                    )

                    # è¯¦ç»†è¾“å‡ºè¯·æ±‚ä¿¡æ¯
                    logger.info(f"ğŸ“¡ [REQUEST] URL: {self.api_url}")
                    logger.info(f"ğŸ“¡ [REQUEST] Model: {model}")
                    logger.info(f"ğŸ“¡ [REQUEST] API Key (first 15 chars): {self.api_key[:15]}...")
                    logger.info(f"ğŸ“¡ [REQUEST] API Key (last 5 chars): ...{self.api_key[-5:]}")
                    logger.info(f"ğŸ“¡ [REQUEST] API Key length: {len(self.api_key)}")

                    # å‘é€è¯·æ±‚
                    async with self.session.post(self.api_url, data=data) as response:
                        chunk_elapsed = time.time() - chunk_start

                        if response.status != 200:
                            error_text = await response.text()
                            logger.error(f"âŒ [CHUNK {chunk.index:03d}] API error (Attempt {attempt+1}): {response.status} - {error_text}")

                            # è®°å½•æœ¬æ¬¡å°è¯•å¤±è´¥
                            if ai_repo and config_db_id:
                                try:
                                    await ai_repo.increment_usage(config_db_id, success=False)
                                    logger.debug(f"ğŸ“Š [STATS] Recorded failure for chunk {chunk.index}, attempt {attempt+1}")
                                except Exception as stats_error:
                                    logger.warning(f"âš ï¸ [STATS] Failed to record failure stats: {stats_error}")

                            if attempt < max_retries - 1:
                                delay = base_delay * (2 ** attempt)
                                logger.info(f"â³ [CHUNK {chunk.index:03d}] Retrying in {delay}s...")
                                await asyncio.sleep(delay)
                                continue
                            else:
                                chunk.transcript = None
                                return chunk

                        result = await response.json()
                        transcript = result.get('text', '')
                        transcript_len = len(transcript)

                        chunk_elapsed = time.time() - chunk_start
                        logger.info(f"âœ… [CHUNK {chunk.index:03d}] Success! Got {transcript_len} chars in {chunk_elapsed:.2f}s")

                        # è®°å½•æœ¬æ¬¡å°è¯•æˆåŠŸ
                        if ai_repo and config_db_id:
                            try:
                                await ai_repo.increment_usage(config_db_id, success=True)
                                logger.debug(f"ğŸ“Š [STATS] Recorded success for chunk {chunk.index}, attempt {attempt+1}")
                            except Exception as stats_error:
                                logger.warning(f"âš ï¸ [STATS] Failed to record success stats: {stats_error}")

                        chunk.transcript = transcript
                        return chunk

                except Exception as e:
                    chunk_elapsed = time.time() - chunk_start
                    logger.error(f"âŒ [CHUNK {chunk.index:03d}] Failed attempt {attempt+1} after {chunk_elapsed:.2f}s: {str(e)}")

                    # è®°å½•æœ¬æ¬¡å°è¯•å¤±è´¥
                    if ai_repo and config_db_id:
                        try:
                            await ai_repo.increment_usage(config_db_id, success=False)
                            logger.debug(f"ğŸ“Š [STATS] Recorded failure for chunk {chunk.index}, attempt {attempt+1} (exception)")
                        except Exception as stats_error:
                            logger.warning(f"âš ï¸ [STATS] Failed to record failure stats: {stats_error}")

                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)
                        logger.info(f"â³ [CHUNK {chunk.index:03d}] Retrying in {delay}s...")
                        await asyncio.sleep(delay)
                    else:
                        chunk.transcript = None
                        return chunk

            return chunk


    async def transcribe_chunks(
        self,
        chunks: list[AudioChunk],
        model: str = "FunAudioLLM/SenseVoiceSmall",
        progress_callback=None,
        ai_repo=None,
        config_db_id: int | None = None
    ) -> list[AudioChunk]:
        """
        å¹¶å‘è½¬å½•å¤šä¸ªéŸ³é¢‘ç‰‡æ®µ

        Args:
            chunks: éŸ³é¢‘åˆ†ç‰‡åˆ—è¡¨
            model: è½¬å½•æ¨¡å‹åç§°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            ai_repo: AIæ¨¡å‹é…ç½®ä»“åº“ï¼ˆç”¨äºè®°å½•ç»Ÿè®¡ï¼‰
            config_db_id: AIæ¨¡å‹é…ç½®æ•°æ®åº“ID

        Note:
            ç»Ÿè®¡è®°å½•åœ¨ transcribe_chunk æ–¹æ³•ä¸­æ¯æ¬¡APIè°ƒç”¨åç«‹å³è¿›è¡Œï¼Œ
            åŒ…æ‹¬é‡è¯•å°è¯•ï¼Œå› æ­¤è¿™é‡Œä¸éœ€è¦é‡å¤è®°å½•ã€‚
        """
        start_time = time.time()

        # åˆ›å»ºè½¬å½•ä»»åŠ¡ï¼ˆä¼ å…¥ ai_repo å’Œ config_db_idï¼‰
        tasks = [
            asyncio.create_task(self.transcribe_chunk(chunk, model, ai_repo, config_db_id))
            for chunk in chunks
        ]

        # æ‰§è¡Œå¹¶å‘è½¬å½•
        results = []
        completed = 0

        for coro in asyncio.as_completed(tasks):
            try:
                # transcribe_chunk now returns the chunk itself
                chunk = await coro
                results.append(chunk)

                completed += 1
                if progress_callback:
                    progress = (completed / len(chunks)) * 100
                    await progress_callback(progress)

            except Exception as e:
                logger.error(f"Unexpected error in transcribe_chunks sequence: {str(e)}")

        duration = time.time() - start_time
        # Ensure correct order
        results.sort(key=lambda x: x.index)

        success_count = sum(1 for c in results if c.transcript is not None)
        logger.info(f"Completed transcription of {success_count}/{len(chunks)} chunks in {duration:.2f}s")

        return results



class PodcastTranscriptionService:
    """æ’­å®¢è½¬å½•ä¸»æœåŠ¡"""

    def __init__(self, db: AsyncSession):
        self.db = db
        # è¿›åº¦ç¼“å­˜ï¼Œå‡å°‘æ•°æ®åº“æ“ä½œé¢‘ç‡
        self._progress_cache: dict[str, dict[str, float]] = {}

        # Get path from settings - use absolute path if configured, otherwise resolve relative path
        temp_dir_config = getattr(settings, 'TRANSCRIPTION_TEMP_DIR', './temp/transcription')
        storage_dir_config = getattr(settings, 'TRANSCRIPTION_STORAGE_DIR', './storage/podcasts')

        # Use configured path directly (supports both absolute and relative)
        # In Docker, these will be absolute paths like /app/temp/transcription
        # In local dev, these will be relative paths that get resolved
        self.temp_dir = os.path.abspath(temp_dir_config)
        self.storage_dir = os.path.abspath(storage_dir_config)

        # Log for debugging (use debug level to reduce noise)
        logger.debug(f"ğŸ“ [TRANSCRIPTION] temp_dir = {self.temp_dir} (from config: {temp_dir_config})")
        logger.debug(f"ğŸ“ [TRANSCRIPTION] storage_dir = {self.storage_dir} (from config: {storage_dir_config})")
        logger.debug(f"ğŸ“ [TRANSCRIPTION] cwd = {os.getcwd()}")

        self.chunk_size_mb = getattr(settings, 'TRANSCRIPTION_CHUNK_SIZE_MB', 10)
        self.max_threads = getattr(settings, 'TRANSCRIPTION_MAX_THREADS', 4)
        # API configuration is now dynamic, but we keep defaults for fallback
        self.default_api_url = getattr(settings, 'TRANSCRIPTION_API_URL', 'https://api.siliconflow.cn/v1/audio/transcriptions')
        self.default_api_key = getattr(settings, 'TRANSCRIPTION_API_KEY', None)

    def _get_episode_storage_path(self, episode: PodcastEpisode) -> str:
        """è·å–æ’­å®¢å•é›†çš„å­˜å‚¨è·¯å¾„"""
        # æ¸…ç†æ’­å®¢åç§°å’Œåˆ†é›†åç§°
        podcast_name = self._sanitize_filename(episode.subscription.title)
        episode_name = self._sanitize_filename(episode.title)

        return os.path.join(
            self.storage_dir,
            podcast_name,
            episode_name
        )

    def _sanitize_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        import re
        # ç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦
        filename = re.sub(r'[<>:"/\\|?*]', '', filename)
        filename = filename.replace(' ', '_')
        return filename[:100]  # é™åˆ¶é•¿åº¦

    def _get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œ"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    async def update_task_progress(
        self,
        task_id: int,
        status: TranscriptionStatus,
        progress: float,
        message: str,
        error_message: str | None = None
    ):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        update_data = {
            'status': status,
            'progress_percentage': progress,
            'updated_at': datetime.now(timezone.utc)
        }

        if error_message:
            update_data['error_message'] = error_message

        # è®¾ç½®å¼€å§‹æ—¶é—´
        if status == 'downloading' and not await self._get_task_field(task_id, 'started_at'):
            update_data['started_at'] = datetime.now(timezone.utc)

        # è®¾ç½®å®Œæˆæ—¶é—´
        if status in [TranscriptionStatus.COMPLETED, TranscriptionStatus.FAILED, TranscriptionStatus.CANCELLED]:
            update_data['completed_at'] = datetime.now(timezone.utc)

        stmt = (
            update(TranscriptionTask)
            .where(TranscriptionTask.id == task_id)
            .values(**update_data)
        )

        await self.db.execute(stmt)
        await self.db.commit()

        # ä½¿ç”¨èŠ‚æµå™¨å‡å°‘æ—¥å¿—è¾“å‡º
        if _progress_throttle.should_log(task_id, str(status), progress):
            logger.info(f"Updated task {task_id}: status={status}, progress={progress:.1f}%")

    async def _get_task_field(self, task_id: int, field: str):
        """è·å–ä»»åŠ¡çš„æŒ‡å®šå­—æ®µ"""
        stmt = select(getattr(TranscriptionTask, field)).where(TranscriptionTask.id == task_id)
        result = await self.db.execute(stmt)
        return result.scalar()

    async def _update_task_progress_with_session(
        self,
        session: AsyncSession,
        task_id: int,
        step: TranscriptionStep,  # ç°åœ¨æ˜¯ step è€Œä¸æ˜¯ status
        progress: float,
        message: str,
        error_message: str | None = None
    ):
        """ä½¿ç”¨æŒ‡å®šçš„æ•°æ®åº“ä¼šè¯æ›´æ–°ä»»åŠ¡è¿›åº¦å’Œæ­¥éª¤"""
        from app.domains.podcast.models import TranscriptionStatus

        # ä½¿ç”¨å†…å­˜ç¼“å­˜å‡å°‘æ•°æ®åº“è¯»å–é¢‘ç‡
        # åªæœ‰å½“è¿›åº¦å˜åŒ–è¶…è¿‡1%æ—¶æ‰çœŸæ­£æ›´æ–°æ•°æ®åº“
        cache_key = f"{task_id}_{step}"
        if cache_key not in self._progress_cache:
            self._progress_cache[cache_key] = {'last_db_update': 0.0, 'last_log': 0.0}

        cached = self._progress_cache[cache_key]
        progress_delta = abs(progress - cached['last_db_update'])

        # åªåœ¨è¿›åº¦å˜åŒ–è¶…è¿‡1%æ—¶æ‰æ›´æ–°æ•°æ®åº“
        if progress_delta < 1.0 and int(progress) != 100:
            return  # è·³è¿‡æ­¤æ¬¡æ›´æ–°

        update_data = {
            'current_step': step,
            'progress_percentage': progress,
            'updated_at': datetime.now(timezone.utc)
        }

        if error_message:
            update_data['error_message'] = error_message

        # è®¾ç½®å¼€å§‹æ—¶é—´ï¼ˆç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶ï¼‰
        stmt_check = select(TranscriptionTask.started_at).where(TranscriptionTask.id == task_id)
        result = await session.execute(stmt_check)
        started_at = result.scalar()
        if not started_at:
            update_data['started_at'] = datetime.now(timezone.utc)
            update_data['status'] = TranscriptionStatus.IN_PROGRESS

        # Try to update chunk_info with the debug message
        if message:
            stmt_info = select(TranscriptionTask.chunk_info).where(TranscriptionTask.id == task_id)
            result_info = await session.execute(stmt_info)
            current_chunk_info = result_info.scalar() or {}

            if not isinstance(current_chunk_info, dict):
                current_chunk_info = {}

            # Update debug_message
            current_chunk_info['debug_message'] = message
            update_data['chunk_info'] = current_chunk_info

        stmt = (
            update(TranscriptionTask)
            .where(TranscriptionTask.id == task_id)
            .values(**update_data)
        )

        await session.execute(stmt)
        await session.commit()

        # æ›´æ–°ç¼“å­˜
        cached['last_db_update'] = progress

        # åŸºäºè¿›åº¦å˜åŒ–åˆ¤æ–­æ˜¯å¦éœ€è¦è®°å½•æ—¥å¿—
        log_delta = abs(progress - cached['last_log'])
        # åªåœ¨è¿›åº¦å˜åŒ–è¶…è¿‡5%æˆ–å®Œæˆæ—¶æ‰è®°å½•æ—¥å¿—
        if log_delta >= 5.0 or int(progress) == 100:
            # ä½¿ç”¨ç®€åŒ–çš„æ—¥å¿—æ ¼å¼
            if int(progress) == 100:
                logger.info(f"âœ… [PROGRESS] Task {task_id}: {step} - COMPLETED")
            else:
                logger.info(f"ğŸ“Š [PROGRESS] Task {task_id}: {step} - {progress:.1f}%")
            cached['last_log'] = progress

    async def _set_task_final_status(
        self,
        session: AsyncSession,
        task_id: int,
        status: TranscriptionStatus,  # COMPLETED æˆ– FAILED
        error_message: str | None = None
    ):
        """è®¾ç½®ä»»åŠ¡çš„æœ€ç»ˆçŠ¶æ€ï¼ˆCOMPLETED æˆ– FAILEDï¼‰"""
        update_data = {
            'status': status,
            'updated_at': datetime.now(timezone.utc)
        }

        if status in [TranscriptionStatus.COMPLETED, TranscriptionStatus.FAILED, TranscriptionStatus.CANCELLED]:
            update_data['completed_at'] = datetime.now(timezone.utc)

        if error_message:
            update_data['error_message'] = error_message

        stmt = (
            update(TranscriptionTask)
            .where(TranscriptionTask.id == task_id)
            .values(**update_data)
        )

        await session.execute(stmt)
        await session.commit()

        logger.info(f"Set task {task_id} final status: {status}")

    async def create_transcription_task_record(self, episode_id: int, model: str | None = None, force: bool = False) -> tuple[TranscriptionTask, int | None]:
        """
        åˆ›å»ºè½¬å½•ä»»åŠ¡è®°å½•ï¼ˆä¸ç«‹å³æ‰§è¡Œï¼‰
        
        Returns:
            Tuple[TranscriptionTask, Optional[int]]: (ä»»åŠ¡å¯¹è±¡, æ¨¡å‹é…ç½®DB ID)
        """
        logger.info(f"ğŸ¬ [TRANSCRIPTION PREPARE] episode_id={episode_id}, model={model}, force={force}")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è½¬å½•ä»»åŠ¡
        stmt = select(TranscriptionTask).where(TranscriptionTask.episode_id == episode_id)
        result = await self.db.execute(stmt)
        existing_task = result.scalar_one_or_none()

        if existing_task:
            logger.info(f"ğŸ”„ [TRANSCRIPTION] Existing task found: id={existing_task.id}, status={existing_task.status}")
            if force:
                # Force mode: delete existing task and create new one (regardless of status)
                logger.info(f"ğŸ—‘ï¸ [TRANSCRIPTION] Force mode: deleting existing task {existing_task.id}")
                await self.db.delete(existing_task)
                await self.db.flush()
                await self.db.commit()  # Commit the delete to release the unique constraint
            elif existing_task.status not in [TranscriptionStatus.FAILED, TranscriptionStatus.CANCELLED]:
                # Task exists with non-failed/cancelled status and force=false: raise error
                logger.warning(f"âš ï¸ [TRANSCRIPTION] Task already exists with status {existing_task.status}")
                raise ValidationError(
                    f"Transcription task already exists for episode {episode_id} with status {existing_task.status}. Use force=true to retry."
                )
            else:
                # Task exists with failed/cancelled status and force=false: delete it and create new one
                logger.info(f"ğŸ—‘ï¸ [TRANSCRIPTION] Removing failed/cancelled task {existing_task.id} before creating new one")
                await self.db.delete(existing_task)
                await self.db.flush()
                await self.db.commit()  # Commit the delete to release the unique constraint
                logger.info("âœ… [TRANSCRIPTION] Failed/cancelled task removed, ready to create new one")

        # è·å–æ’­å®¢å•é›†ä¿¡æ¯
        stmt = select(PodcastEpisode).where(PodcastEpisode.id == episode_id)
        result = await self.db.execute(stmt)
        episode = result.scalar_one_or_none()

        if not episode:
            logger.error(f"âŒ [TRANSCRIPTION] Episode {episode_id} not found")
            raise ValidationError(f"Episode {episode_id} not found")

        logger.info(f"ğŸ“º [TRANSCRIPTION] Episode found: title='{episode.title}', audio_url='{episode.audio_url}'")

        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        ai_repo = AIModelConfigRepository(self.db)

        # 1. å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œå°è¯•æŸ¥æ‰¾
        model_config = None
        if model:
            model_config = await ai_repo.get_by_name(model)
            logger.info(f"ğŸ” [TRANSCRIPTION] Looking for model by name '{model}': {model_config is not None}")
            # æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”æ´»è·ƒ
            if not model_config or not model_config.is_active or model_config.model_type != ModelType.TRANSCRIPTION:
                raise ValidationError(f"Transcription model '{model}' not found or not active")

        # 2. å¦‚æœæœªæŒ‡å®šæˆ–æœªæ‰¾åˆ°ï¼ŒæŒ‰ä¼˜å…ˆçº§è·å–è½¬å½•æ¨¡å‹
        if not model_config:
            active_models = await ai_repo.get_active_models_by_priority(ModelType.TRANSCRIPTION)
            if active_models:
                model_config = active_models[0]  # ä½¿ç”¨ä¼˜å…ˆçº§æœ€é«˜çš„æ¨¡å‹
                logger.info(f"ğŸ” [TRANSCRIPTION] Using highest priority model: {model_config.model_id} (priority={model_config.priority})")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ´»è·ƒçš„è½¬å½•æ¨¡å‹ï¼ŒæŠ›å‡ºé”™è¯¯
                raise ValidationError("No active transcription model found")

        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„æ¨¡å‹IDå­—ç¬¦ä¸² (ä¼ é€’ç»™APIçš„modelå‚æ•°)
        transcription_model = model_config.model_id
        logger.info(f"ğŸ¤– [TRANSCRIPTION] Final model to use: '{transcription_model}'")

        # åˆ›å»ºæ–°çš„è½¬å½•ä»»åŠ¡
        logger.info("ğŸ“ [TRANSCRIPTION] Creating TranscriptionTask in database...")
        task = TranscriptionTask(
            episode_id=episode_id,
            original_audio_url=episode.audio_url,
            chunk_size_mb=self.chunk_size_mb,
            model_used=transcription_model  # è¿™é‡Œå­˜å‚¨çš„æ˜¯APIæ¨¡å‹ID (å¦‚ whisper-1)ï¼Œä¸æ˜¯æ•°æ®åº“ID
        )

        self.db.add(task)
        await self.db.commit()
        await self.db.refresh(task)

        logger.info(f"âœ… [TRANSCRIPTION] Task created in DB: id={task.id}, status={task.status}")

        config_db_id = model_config.id if model_config else None
        return task, config_db_id

    async def start_transcription(self, episode_id: int, model: str | None = None, force: bool = False) -> TranscriptionTask:
        """å¯åŠ¨è½¬å½•ä»»åŠ¡"""
        # 1. åˆ›å»ºä»»åŠ¡è®°å½•
        task, config_db_id = await self.create_transcription_task_record(episode_id, model=model, force=force)

        logger.info(f"ğŸ¯ [TRANSCRIPTION] Task {task.id} created successfully. config_db_id={config_db_id}")

        return task


    async def execute_transcription_task(self, task_id: int, session, config_db_id: int | None = None):
        """æ‰§è¡Œè½¬å½•ä»»åŠ¡ï¼ˆåå°è¿è¡Œï¼‰"""
        log_with_timestamp("INFO", "ğŸ¬ [EXECUTE START] Transcription task starting...", task_id)
        log_with_timestamp("INFO", f"ğŸ“‹ [EXECUTE] config_db_id={config_db_id}", task_id)
        log_with_timestamp("INFO", f"ğŸ“‹ [EXECUTE] asyncio event loop running: {asyncio.get_event_loop().is_running()}", task_id)

        try:
            logger.info(f"ğŸ”— [EXECUTE] Using provided database session for task {task_id}")

            # åˆå§‹åŒ– AI æ¨¡å‹é…ç½®ä»“åº“ï¼ˆç”¨äºè®°å½•ç»Ÿè®¡ï¼‰
            ai_repo = AIModelConfigRepository(session)
            # è·å–ä»»åŠ¡ä¿¡æ¯
            stmt = select(TranscriptionTask).where(TranscriptionTask.id == task_id)
            result = await session.execute(stmt)
            task = result.scalar_one_or_none()

            if not task:
                logger.error(f"âŒ [EXECUTE] Transcription task {task_id} not found in database")
                return

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²ç»å®Œæˆï¼Œé¿å…é‡å¤æ‰§è¡Œ
            if task.status == TranscriptionStatus.COMPLETED:
                log_with_timestamp("INFO", f"âœ… [SKIP] Task {task_id} already completed, skipping execution", task_id)
                log_with_timestamp("INFO", f"ğŸ“„ [SKIP] Transcript has {task.transcript_word_count or 0} words", task_id)
                return

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å–æ¶ˆæˆ–å¤±è´¥ä¸”ä¸åº”é‡è¯•
            if task.status == TranscriptionStatus.CANCELLED:
                log_with_timestamp("WARNING", f"âš ï¸ [SKIP] Task {task_id} was cancelled, skipping execution", task_id)
                return

            # è·å–æ’­å®¢å•é›†ä¿¡æ¯ (é¢„åŠ è½½subscriptionå…³ç³»ä»¥é¿å…lazy load)
            from sqlalchemy.orm import selectinload
            stmt = select(PodcastEpisode).options(
                selectinload(PodcastEpisode.subscription)
            ).where(PodcastEpisode.id == task.episode_id)
            result = await session.execute(stmt)
            episode = result.scalar_one_or_none()

            if not episode:
                logger.error(f"transcription._execute_transcription: Episode {task.episode_id} not found for task {task_id}")
                await self._set_task_final_status(
                    session, task_id,
                    TranscriptionStatus.FAILED,
                    "Episode not found"
                )
                return

            # è·å–è½¬å½•é…ç½®
            api_url = self.default_api_url
            api_key = self.default_api_key

            if config_db_id:
                logger.info(f"transcription._execute_transcription: Using custom model config {config_db_id}")
                model_config = await ai_repo.get_by_id(config_db_id)
                if model_config and model_config.is_active:
                    api_url = model_config.api_url
                    # è·å–API Key - æ”¯æŒåŠ å¯†è§£å¯†
                    if model_config.is_system and model_config.provider == 'siliconflow':
                         api_key = getattr(settings, 'TRANSCRIPTION_API_KEY', None) or model_config.api_key
                    elif model_config.is_system and model_config.provider == 'openai':
                         api_key = getattr(settings, 'OPENAI_API_KEY', None) or model_config.api_key
                    else:
                         # ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹ - éœ€è¦è§£å¯†
                         if model_config.api_key_encrypted and model_config.api_key:
                             from app.core.security import decrypt_data
                             try:
                                 api_key = decrypt_data(model_config.api_key)
                                 logger.info(f"ğŸ”‘ [KEY] Decrypted API key for model {model_config.name} (first 10 chars): {api_key[:10]}...")
                             except Exception as e:
                                 logger.error(f"Failed to decrypt API key: {e}")
                                 api_key = model_config.api_key
                         else:
                             api_key = model_config.api_key

            if not api_key:
                 logger.error(f"transcription._execute_transcription: API Key missing for task {task_id}")
                 await self._set_task_final_status(
                    session, task_id,
                    TranscriptionStatus.FAILED,
                    "Transcription API Key not found"
                )
                 return

            # åˆ›å»ºä¸´æ—¶ç›®å½•
            temp_episode_dir = os.path.join(self.temp_dir, f"episode_{task.episode_id}")
            os.makedirs(temp_episode_dir, exist_ok=True)
            logger.info(f"transcription._execute_transcription: Created temp dir {temp_episode_dir}")

            # === æ­¥éª¤è·³è¿‡é€»è¾‘ï¼šæ ¹æ® current_step å†³å®šä»å“ªä¸€æ­¥å¼€å§‹ ===
            start_step = task.current_step
            log_with_timestamp("INFO", f"ğŸ“ [RESUME] Current step: {start_step}, will resume from this step", task_id)

            # æ­¥éª¤æ‰§è¡Œé¡ºåºï¼šDOWNLOADING -> CONVERTING -> SPLITTING -> TRANSCRIBING -> MERGING
            # å¦‚æœ current_step åœ¨æŸä¸ªæ­¥éª¤ä¹‹åï¼Œå‰é¢çš„æ­¥éª¤å°†è¢«è·³è¿‡

            # === æ­¥éª¤1ï¼šä¸‹è½½éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒå¢é‡æ¢å¤ï¼‰ ===
            download_start = time.time()
            download_time = 0
            original_file = os.path.join(temp_episode_dir, f"original{os.path.splitext(task.original_audio_url)[-1]}")
            file_size = 0

            # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
            if os.path.exists(original_file) and os.path.getsize(original_file) > 0:
                file_size = os.path.getsize(original_file)
                log_with_timestamp("INFO", f"â­ï¸ [STEP 1/6 DOWNLOAD] Skip! File already exists: {original_file} ({file_size/1024/1024:.2f} MB)", task_id)
                log_with_timestamp("INFO", "âœ… [STEP 1/6 DOWNLOAD] Using existing downloaded file", task_id)
            else:
                log_with_timestamp("INFO", "ğŸ“¥ [STEP 1/6 DOWNLOAD] Starting audio download with fallback...", task_id)
                log_with_timestamp("INFO", f"ğŸ“¥ [STEP 1/6 DOWNLOAD] Source URL: {task.original_audio_url[:100]}...", task_id)
                await self._update_task_progress_with_session(
                    session,
                    task_id,
                    'downloading',
                    5,
                    "Downloading audio file..."
                )

                logger.info(f"ğŸ“¥ [STEP 1 DOWNLOAD] Target path: {original_file}")

                async with AudioDownloader() as downloader:
                    # ä½¿ç”¨èŠ‚æµå™¨å‡å°‘æ—¥å¿—
                    last_dl_progress = 0.0

                    async def download_progress(progress):
                        nonlocal last_dl_progress

                        # æ¯10%è®°å½•ä¸€æ¬¡ä¸‹è½½æ—¥å¿—
                        if int(progress) // 10 > int(last_dl_progress) // 10:
                            logger.info(f"ğŸ“¥ [STEP 1 DOWNLOAD] Progress: {progress:.1f}%")
                            last_dl_progress = progress

                        await self._update_task_progress_with_session(
                            session,
                            task_id,
                            'downloading',
                            5 + (progress * 0.15),  # 5-20%
                            f"Downloading... {progress:.1f}%"
                        )

                    # ä½¿ç”¨å¸¦å›é€€æœºåˆ¶çš„ä¸‹è½½æ–¹æ³•
                    file_path, file_size = await downloader.download_file_with_fallback(
                        task.original_audio_url,
                        original_file,
                        download_progress
                    )

                log_with_timestamp("INFO", f"âœ… [STEP 1/6 DOWNLOAD] Download complete! Size: {file_size} bytes ({file_size/1024/1024:.2f} MB)", task_id)
                download_time = time.time() - download_start
                log_with_timestamp("INFO", f"â±ï¸ [STEP 1/6 DOWNLOAD] Time taken: {download_time:.2f}s", task_id)

            file_path = original_file  # ç¡®ä¿file_pathæŒ‡å‘æ­£ç¡®çš„æ–‡ä»¶

            # === æ­¥éª¤2ï¼šè½¬æ¢ä¸ºMP3ï¼ˆæ”¯æŒå¢é‡æ¢å¤ï¼‰ ===
            conversion_time = 0
            converted_file = os.path.join(temp_episode_dir, "converted.mp3")

            log_with_timestamp("INFO", f"ğŸ” [STEP 2/6 CONVERT] Checking conversion status: {converted_file}", task_id)
            log_with_timestamp("INFO", f"ğŸ” [STEP 2/6 CONVERT] File exists: {os.path.exists(converted_file)}", task_id)

            # æ£€æŸ¥æ˜¯å¦å·²è½¬æ¢ï¼ˆæ›´ä¸¥æ ¼çš„éªŒè¯ï¼‰
            skip_conversion = False
            if os.path.exists(converted_file):
                converted_size = os.path.getsize(converted_file)
                log_with_timestamp("INFO", f"ğŸ” [STEP 2/6 CONVERT] Found existing file: {converted_size} bytes", task_id)
                # éªŒè¯æ–‡ä»¶å¤§å°åˆç†ï¼ˆè‡³å°‘10KBï¼Œä¸”ä¸è¶…è¿‡åŸå§‹æ–‡ä»¶å¤ªå¤šï¼‰
                if converted_size > 10240:  # è‡³å°‘10KB
                    # å°è¯•ç”¨ffmpegéªŒè¯æ–‡ä»¶æ˜¯å¦æ˜¯æœ‰æ•ˆçš„MP3
                    try:
                        import ffmpeg
                        probe = ffmpeg.probe(converted_file)
                        log_with_timestamp("INFO", f"ğŸ” [STEP 2/6 CONVERT] FFmpeg probe result: {probe}", task_id)
                        duration = probe.get('format', {}).get('duration') if probe else None
                        if duration:
                            skip_conversion = True
                            log_with_timestamp("INFO", f"â­ï¸ [STEP 2/6 CONVERT] Skip! Valid MP3 file already exists: {converted_file} ({converted_size/1024/1024:.2f} MB, {duration}s)", task_id)
                            log_with_timestamp("INFO", "âœ… [STEP 2/6 CONVERT] Using existing converted file", task_id)
                        else:
                            log_with_timestamp("WARNING", f"âš ï¸ [STEP 2/6 CONVERT] File exists but invalid (no duration), re-converting: {converted_file}", task_id)
                    except Exception as e:
                        log_with_timestamp("WARNING", f"âš ï¸ [STEP 2/6 CONVERT] File exists but validation failed ({str(e)}), re-converting", task_id)
                    else:
                        log_with_timestamp("WARNING", f"âš ï¸ [STEP 2/6 CONVERT] File exists but too small ({converted_size} bytes), re-converting", task_id)
                else:
                    log_with_timestamp("INFO", "ğŸ” [STEP 2/6 CONVERT] File does not exist, will convert", task_id)

            if not skip_conversion:
                log_with_timestamp("INFO", "ğŸ”„ [STEP 2/6 CONVERT] Starting MP3 conversion...", task_id)
                await self._update_task_progress_with_session(
                    session,
                    task_id,
                    'converting',
                    20,
                    "Converting to MP3..."
                )

                async def convert_progress(progress):
                    await self._update_task_progress_with_session(
                        session,
                        task_id,
                        'converting',
                        20 + (progress * 0.15),  # 20-35%
                        f"Converting... {progress:.1f}%"
                    )

                convert_start = time.time()
                _, conversion_time = await AudioConverter.convert_to_mp3(
                    file_path,
                    converted_file,
                    convert_progress
                )

                # Verify the converted file was actually created
                if not os.path.exists(converted_file):
                    error_msg = f"Conversion completed but output file not found: {converted_file}"
                    logger.error(f"âŒ [STEP 2/6 CONVERT] {error_msg}")
                    logger.error(f"âŒ [STEP 2/6 CONVERT] Input file: {file_path}, exists: {os.path.exists(file_path)}")
                    await self._set_task_final_status(
                        session, task_id,
                        TranscriptionStatus.FAILED,
                        "MP3 conversion failed - output file not created"
                    )
                    return

                converted_size = os.path.getsize(converted_file)
                log_with_timestamp("INFO", f"âœ… [STEP 2/6 CONVERT] Conversion complete! Output: {converted_file} ({converted_size/1024/1024:.2f} MB), Time: {conversion_time:.2f}s", task_id)

            # Final verification before moving to STEP 3
            log_with_timestamp("INFO", f"ğŸ” [STEP 2->3] Final check: converted_file exists = {os.path.exists(converted_file)}, size = {os.path.getsize(converted_file) if os.path.exists(converted_file) else 0}", task_id)

            # === æ­¥éª¤3ï¼šåˆ‡å‰²éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒå¢é‡æ¢å¤ï¼‰ ===
            # é¦–å…ˆéªŒè¯converted_fileç¡®å®å­˜åœ¨ä¸”æœ‰æ•ˆ
            log_with_timestamp("INFO", "ğŸ“‹ [STEP 3/6 SPLIT] Starting split verification...", task_id)

            if not os.path.exists(converted_file):
                error_msg = f"Converted file not found: {converted_file}. Cannot proceed with split."
                logger.error(f"âŒ [STEP 3/6 SPLIT] {error_msg}")
                logger.error(f"âŒ [STEP 3/6 SPLIT] Working directory: {os.getcwd()}")
                logger.error(f"âŒ [STEP 3/6 SPLIT] Temp dir exists: {os.path.exists(temp_episode_dir)}")
                if os.path.exists(temp_episode_dir):
                    files = os.listdir(temp_episode_dir)
                    logger.error(f"âŒ [STEP 3/6 SPLIT] Files in temp dir: {files}")
                await self._set_task_final_status(
                    session, task_id,
                    TranscriptionStatus.FAILED,
                    "Converted audio file missing, cannot split"
                )
                return

            converted_file_size = os.path.getsize(converted_file)
            if converted_file_size == 0:
                error_msg = f"Converted file is empty: {converted_file}. Cannot proceed with split."
                logger.error(f"âŒ [STEP 3/6 SPLIT] {error_msg}")
                await self._set_task_final_status(
                    session, task_id,
                    TranscriptionStatus.FAILED,
                    "Converted audio file is empty, cannot split"
                )
                return

            log_with_timestamp("INFO", f"ğŸ“‹ [STEP 3/6 SPLIT] Verified converted file exists: {converted_file} ({converted_file_size/1024/1024:.2f} MB)", task_id)

            split_dir = os.path.join(temp_episode_dir, "chunks")

            # æ£€æŸ¥æ˜¯å¦å·²åˆ†å‰²
            if os.path.exists(split_dir) and os.path.isdir(split_dir):
                # æ£€æŸ¥æ˜¯å¦æœ‰chunkæ–‡ä»¶
                chunk_files = [f for f in os.listdir(split_dir) if f.startswith('chunk_') and f.endswith('.mp3')]
                if chunk_files:
                    log_with_timestamp("INFO", f"â­ï¸ [STEP 3/6 SPLIT] Skip! Chunks already exist: {len(chunk_files)} files found", task_id)
                    log_with_timestamp("INFO", "âœ… [STEP 3/6 SPLIT] Using existing chunks", task_id)
                    # é‡å»ºchunkså¯¹è±¡åˆ—è¡¨
                    chunks = []
                    for chunk_file in sorted(chunk_files):
                        chunk_path = os.path.join(split_dir, chunk_file)
                        # ä»æ–‡ä»¶åè§£æchunkä¿¡æ¯ (chunk_0001.mp3 -> index=1)
                        index = int(chunk_file.replace('chunk_', '').replace('.mp3', ''))
                        file_size = os.path.getsize(chunk_path)
                        chunks.append(AudioChunk(
                            index=index,
                            file_path=chunk_path,
                            start_time=0,  # è¿™äº›ä¿¡æ¯ä¼šä»æ–‡ä»¶ä¸­è·å–
                            duration=0,
                            file_size=file_size,
                            transcript=None
                        ))
                else:
                    # éœ€è¦æ‰§è¡Œåˆ†å‰²
                    log_with_timestamp("INFO", f"âœ‚ï¸ [STEP 3/6 SPLIT] Starting audio split with chunk_size_mb={task.chunk_size_mb}...", task_id)
                    await self._update_task_progress_with_session(
                        session,
                        task_id,
                        'splitting',
                        35,
                        "Splitting audio file..."
                    )

                    async def split_progress(progress):
                        await self._update_task_progress_with_session(
                            session,
                            task_id,
                            'splitting',
                            35 + (progress * 0.10),  # 35-45%
                            f"Splitting... {progress:.1f}%"
                        )

                    chunks = await AudioSplitter.split_mp3(
                        converted_file,
                        split_dir,
                        task.chunk_size_mb,
                        split_progress
                    )
                    log_with_timestamp("INFO", f"âœ… [STEP 3/6 SPLIT] Split complete! Created {len(chunks)} chunks", task_id)
            else:
                # éœ€è¦æ‰§è¡Œåˆ†å‰²
                log_with_timestamp("INFO", f"âœ‚ï¸ [STEP 3/6 SPLIT] Starting audio split with chunk_size_mb={task.chunk_size_mb}...", task_id)
                await self._update_task_progress_with_session(
                    session,
                    task_id,
                    'splitting',
                    35,
                    "Splitting audio file..."
                )

                async def split_progress(progress):
                    await self._update_task_progress_with_session(
                        session,
                        task_id,
                        'splitting',
                        35 + (progress * 0.10),  # 35-45%
                        f"Splitting... {progress:.1f}%"
                    )

                chunks = await AudioSplitter.split_mp3(
                    converted_file,
                    split_dir,
                    task.chunk_size_mb,
                    split_progress
                )
                log_with_timestamp("INFO", f"âœ… [STEP 3/6 SPLIT] Split complete! Created {len(chunks)} chunks", task_id)

            # === æ­¥éª¤4ï¼šè½¬å½•éŸ³é¢‘ç‰‡æ®µï¼ˆæ”¯æŒå¢é‡æ¢å¤ï¼‰ ===
            # æ£€æŸ¥æ˜¯å¦æœ‰å·²è½¬å½•çš„ç‰‡æ®µ
            chunks_to_transcribe = []
            already_transcribed = []
            for chunk in chunks:
                transcript_file = chunk.file_path.replace('.mp3', '.txt')
                if os.path.exists(transcript_file) and os.path.getsize(transcript_file) > 0:
                    # åŠ è½½å·²æœ‰çš„è½¬å½•
                    async with aiofiles.open(transcript_file, encoding='utf-8') as f:
                        content = await f.read()
                    if content.strip():
                        chunk.transcript = content
                        already_transcribed.append(chunk)
                else:
                    chunks_to_transcribe.append(chunk)

            if already_transcribed:
                log_with_timestamp("INFO", f"â­ï¸ [STEP 4/6 TRANSCRIBE] Found {len(already_transcribed)} already transcribed chunks, skipping", task_id)

            log_with_timestamp("INFO", f"ğŸ¤– [STEP 4/6 TRANSCRIBE] Starting transcription of {len(chunks_to_transcribe)} remaining chunks...", task_id)
            log_with_timestamp("INFO", f"ğŸ¤– [STEP 4/6 TRANSCRIBE] Model: {task.model_used}", task_id)

            if chunks_to_transcribe:
                await self._update_task_progress_with_session(
                    session,
                    task_id,
                    'transcribing',
                    45,
                    f"Transcribing {len(chunks_to_transcribe)} audio chunks..."
                )

                transcription_start = time.time()

                # ä½¿ç”¨èŠ‚æµå™¨å‡å°‘æ—¥å¿—
                last_trans_progress = 0.0

                async def transcribe_progress(progress):
                    nonlocal last_trans_progress

                    # æ¯10%è®°å½•ä¸€æ¬¡è½¬å½•æ—¥å¿—
                    if int(progress) // 10 > int(last_trans_progress) // 10:
                        logger.info(f"ğŸ¤– [STEP 4 TRANSCRIBE] Progress: {progress:.1f}%")
                        last_trans_progress = progress

                    await self._update_task_progress_with_session(
                        session,
                        task_id,
                        'transcribing',
                        45 + (progress * 0.50),  # 45-95%
                        f"Transcribing... {progress:.1f}%"
                    )

                async with SiliconFlowTranscriber(
                    api_key,
                    api_url,
                    self.max_threads
                ) as transcriber:
                    transcribed_chunks = await transcriber.transcribe_chunks(
                        chunks_to_transcribe,
                        task.model_used,
                        transcribe_progress,
                        ai_repo=ai_repo,
                        config_db_id=config_db_id
                    )

                # åˆå¹¶å·²æœ‰è½¬å½•å’Œæ–°è½¬å½•
                all_chunks = already_transcribed + transcribed_chunks

                log_with_timestamp("INFO", "âœ… [STEP 4/6 TRANSCRIBE] Transcription chunks finished!", task_id)

                # Log transcription results summary
                success_count = sum(1 for c in all_chunks if c.transcript)
                failed_count = len(all_chunks) - success_count
                log_with_timestamp("INFO", f"ğŸ“Š [STEP 4/6 TRANSCRIBE] Results: {success_count} succeeded, {failed_count} failed out of {len(all_chunks)} total", task_id)

                transcription_time = time.time() - transcription_start
                log_with_timestamp("INFO", f"â±ï¸ [STEP 4/6 TRANSCRIBE] Time taken: {transcription_time:.2f}s", task_id)
            else:
                # æ‰€æœ‰ç‰‡æ®µéƒ½å·²è½¬å½•
                all_chunks = already_transcribed
                log_with_timestamp("INFO", "âœ… [STEP 4/6 TRANSCRIBE] All chunks already transcribed! Skipping transcription", task_id)
                success_count = len(all_chunks)
                failed_count = 0
                transcription_time = 0

            # æ­¥éª¤5ï¼šåˆå¹¶è½¬å½•ç»“æœ
            log_with_timestamp("INFO", "ğŸ”— [STEP 5/6 MERGE] Merging transcription results...", task_id)
            await self._update_task_progress_with_session(
                session,
                task_id,
                'merging',
                95,
                "Merging transcription results..."
            )

            # æŒ‰é¡ºåºåˆå¹¶è½¬å½•æ–‡æœ¬
            sorted_chunks = sorted(all_chunks, key=lambda x: x.index)
            full_transcript = "\n\n".join([
                chunk.transcript.strip() for chunk in sorted_chunks
                if chunk.transcript and chunk.transcript.strip()
            ])

            log_with_timestamp("INFO", f"ğŸ“„ [STEP 5/6 MERGE] Merged transcript: {len(full_transcript)} chars, {len(full_transcript.split())} words", task_id)
            log_with_timestamp("INFO", f"ğŸ“„ [STEP 5/6 MERGE] Preview: {full_transcript[:150]}...", task_id)

            # æ­¥éª¤6ï¼šä¿å­˜ç»“æœåˆ°æ°¸ä¹…å­˜å‚¨
            storage_path = self._get_episode_storage_path(episode)
            os.makedirs(storage_path, exist_ok=True)

            # ä¿å­˜åŸå§‹éŸ³é¢‘æ–‡ä»¶
            final_audio_path = os.path.join(storage_path, "original.mp3")

            # Verify converted file exists before copying
            if not os.path.exists(converted_file):
                error_msg = f"Converted audio file not found: {converted_file}"
                logger.error(f"âŒ [STEP 6 SAVE] {error_msg}")
                logger.error(f"âŒ [STEP 6 SAVE] Working directory: {os.getcwd()}")
                logger.error(f"âŒ [STEP 6 SAVE] Absolute path: {os.path.abspath(converted_file)}")
                # List files in temp directory for debugging
                if os.path.exists(temp_episode_dir):
                    files = os.listdir(temp_episode_dir)
                    logger.error(f"âŒ [STEP 6 SAVE] Files in temp dir: {files}")
                else:
                    logger.error(f"âŒ [STEP 6 SAVE] Temp directory does not exist: {temp_episode_dir}")
                raise FileNotFoundError(error_msg)

            # Move audio file to permanent storage
            # Use shutil.move instead of os.replace to handle cross-device moves (e.g., Docker volumes)
            # ä½¿ç”¨ shutil.move è€Œé os.replaceï¼Œä»¥å¤„ç†è·¨è®¾å¤‡ç§»åŠ¨ï¼ˆå¦‚ Docker å·ï¼‰
            import shutil
            try:
                shutil.move(converted_file, final_audio_path)
            except OSError as e:
                logger.warning(f"âš ï¸ [STEP 6 SAVE] shutil.move failed ({e}), trying copy + delete")
                shutil.copy2(converted_file, final_audio_path)
                try:
                    os.remove(converted_file)
                except OSError:
                    logger.warning(f"âš ï¸ [STEP 6 SAVE] Could not remove source file: {converted_file}")

            # ä¿å­˜è½¬å½•æ–‡æœ¬
            transcript_path = os.path.join(storage_path, "transcript.txt")
            async with aiofiles.open(transcript_path, 'w', encoding='utf-8') as f:
                await f.write(full_transcript)

            log_with_timestamp("INFO", f"ğŸ’¾ [STEP 6/6 SAVE] Transcript saved to: {transcript_path}", task_id)

            # æ›´æ–°ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
            task_update = {
                'status': TranscriptionStatus.COMPLETED,
                'current_step': 'merging',  # ä¿æŒæœ€åçš„æ­¥éª¤
                'progress_percentage': 100.0,
                'transcript_content': full_transcript,
                'transcript_word_count': len(full_transcript.split()),
                'original_file_path': final_audio_path,
                'original_file_size': file_size,
                'download_time': download_time,
                'conversion_time': conversion_time,
                'transcription_time': transcription_time,
                'chunk_info': {
                    'total_chunks': len(chunks),
                    'chunks': [
                        {
                            'index': chunk.index,
                            'start_time': chunk.start_time,
                            'duration': chunk.duration,
                            'transcript': chunk.transcript
                        }
                        for chunk in sorted_chunks
                    ]
                },
                'completed_at': datetime.now(timezone.utc)
            }

            stmt = (
                update(TranscriptionTask)
                .where(TranscriptionTask.id == task_id)
                .values(**task_update)
            )
            await session.execute(stmt)

            # æ›´æ–°æ’­å®¢å•é›†çš„è½¬å½•ä¿¡æ¯
            episode_update = {
                'transcript_content': full_transcript,
                'transcript_url': f"file://{transcript_path}",
                'status': 'completed'
            }

            stmt = (
                update(PodcastEpisode)
                .where(PodcastEpisode.id == task.episode_id)
                .values(**episode_update)
            )
            await session.execute(stmt)

            await session.commit()

            total_time = time.time() - download_start
            log_with_timestamp("INFO", f"âœ… [TRANSCRIPTION COMPLETE] Successfully completed transcription for episode {task.episode_id}", task_id)
            log_with_timestamp("INFO", f"âœ… [TRANSCRIPTION COMPLETE] Total time: {total_time:.2f}s (download:{download_time:.2f}s, convert:{conversion_time:.2f}s, transcribe:{transcription_time:.2f}s)", task_id)
            log_with_timestamp("INFO", f"âœ… [TRANSCRIPTION COMPLETE] Transcript: {len(full_transcript)} chars, {len(full_transcript.split())} words", task_id)

            # è§¦å‘AIæ€»ç»“
            log_with_timestamp("INFO", f"ğŸ¤– [AI SUMMARY] Scheduling AI summary for episode {task.episode_id}", task_id)
            await self._schedule_ai_summary(session, task_id)
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            logger.error(f"âŒ [EXECUTE ERROR] Transcription failed for task {task_id}: {str(e)}")
            logger.error(f"âŒ [EXECUTE ERROR] Traceback:\n{error_trace}")
            await self._set_task_final_status(
                session,
                task_id,
                TranscriptionStatus.FAILED,
                f"Transcription failed: {str(e)}"
            )
        finally:
            # Only clean up temporary files if the task completed successfully
            # Failed or interrupted tasks should keep their temp files for incremental recovery
            try:
                # Re-fetch task status to see if it completed successfully
                stmt_check = select(TranscriptionTask.status).where(TranscriptionTask.id == task_id)
                result_check = await session.execute(stmt_check)
                final_status = result_check.scalar()

                if final_status == TranscriptionStatus.COMPLETED:
                    import shutil
                    temp_episode_dir = os.path.join(self.temp_dir, f"episode_{task.episode_id}")
                    if os.path.exists(temp_episode_dir):
                        shutil.rmtree(temp_episode_dir)
                        logger.info(f"ğŸ§¹ [CLEANUP] Cleaned up temporary directory for successful task {task_id}: {temp_episode_dir}")
                else:
                    temp_episode_dir = os.path.join(self.temp_dir, f"episode_{task.episode_id}")
                    if os.path.exists(temp_episode_dir):
                        logger.info(f"â¸ï¸ [CLEANUP] Preserving temporary directory for task {task_id} (status={final_status}): {temp_episode_dir}")
            except Exception as e:
                logger.error(f"âš ï¸ [CLEANUP] Error during cleanup: {str(e)}")

    async def get_transcription_status(self, task_id: int) -> TranscriptionTask | None:
        """è·å–è½¬å½•ä»»åŠ¡çŠ¶æ€"""
        stmt = select(TranscriptionTask).where(TranscriptionTask.id == task_id)
        result = await self.db.execute(stmt)
        return result.scalar_one_or_none()

    async def get_episode_transcription(self, episode_id: int) -> TranscriptionTask | None:
        """è·å–æ’­å®¢å•é›†çš„è½¬å½•ä¿¡æ¯"""
        stmt = select(TranscriptionTask).where(TranscriptionTask.episode_id == episode_id)
        result = await self.db.execute(stmt)
        return result.scalar_one_or_none()

    async def _schedule_ai_summary(self, session: AsyncSession, task_id: int):
        """è°ƒåº¦AIæ€»ç»“ä»»åŠ¡"""
        import logging
        logger = logging.getLogger(__name__)
        
        try:
            # è·å–è½¬å½•ä»»åŠ¡
            log_with_timestamp("INFO", f"ğŸ” [AI SUMMARY] Getting transcription task {task_id}", task_id)
            stmt = select(TranscriptionTask).where(TranscriptionTask.id == task_id)
            result = await session.execute(stmt)
            task = result.scalar_one_or_none()
            
            if not task:
                log_with_timestamp("ERROR", f"âŒ [AI SUMMARY] Transcription task {task_id} not found", task_id)
                return
            
            log_with_timestamp("INFO", f"âœ… [AI SUMMARY] Found transcription task {task_id} for episode {task.episode_id}", task_id)
            
            # ä½¿ç”¨DatabaseBackedAISummaryServiceç”Ÿæˆæ€»ç»“
            summary_service = DatabaseBackedAISummaryService(session)
            log_with_timestamp("INFO", f"ğŸ¤– [AI SUMMARY] Starting AI summary generation for episode {task.episode_id}", task_id)
            
            # è°ƒç”¨AIæ€»ç»“æœåŠ¡
            summary_result = await summary_service.generate_summary(task.episode_id)

            # è®¡ç®—å­—æ•°
            word_count = len(summary_result['summary_content'].split())

            log_with_timestamp("INFO", f"âœ… [AI SUMMARY] Successfully generated summary for episode {task.episode_id}", task_id)
            log_with_timestamp("INFO", f"âœ… [AI SUMMARY] Summary: {len(summary_result['summary_content'])} chars, {word_count} words", task_id)
            log_with_timestamp("INFO", f"âœ… [AI SUMMARY] Processing time: {summary_result['processing_time']:.2f}s, Model: {summary_result['model_name']}", task_id)

            # ğŸ”¥ å…³é”®ä¿®å¤: åˆ·æ–°sessionä¸­çš„taskå¯¹è±¡ï¼Œç¡®ä¿AIæ‘˜è¦ç«‹å³å¯è§
            # è¿™æ˜¯å› ä¸º summary_service.generate_summary() å†…éƒ¨ä½¿ç”¨äº†ç‹¬ç«‹çš„db sessionæäº¤
            # æˆ‘ä»¬éœ€è¦åˆ·æ–°å½“å‰sessionä¸­çš„taskå¯¹è±¡
            try:
                await session.refresh(task)
                log_with_timestamp("INFO", "ğŸ”„ [AI SUMMARY] Refreshed task object from database, summary_content is now available", task_id)
            except Exception as refresh_error:
                log_with_timestamp("WARNING", f"âš ï¸ [AI SUMMARY] Failed to refresh task: {refresh_error}", task_id)
            
        except Exception as e:
            import traceback
            error_trace = traceback.format_exc()
            error_msg = str(e)
            log_with_timestamp("ERROR", f"âŒ [AI SUMMARY] Failed to generate summary for task {task_id}: {error_msg}", task_id)
            logger.error(f"âŒ [AI SUMMARY] Traceback: {error_trace}")
            
            # ä¸è¦å°è¯•åœ¨åŒä¸€ä¸ªä¼šè¯ä¸­å†æ¬¡æäº¤ï¼Œå› ä¸ºå‰é¢å¯èƒ½å·²ç»æäº¤æˆ–å›æ»šäº†
            # è¿™é‡Œåªè®°å½•é”™è¯¯ï¼Œä¸ä¿®æ”¹æ•°æ®åº“
            log_with_timestamp("ERROR", f"âŒ [AI SUMMARY] Cannot update task {task_id} with error info in current transaction", task_id)
    
    async def cancel_transcription(self, task_id: int) -> bool:
        """å–æ¶ˆè½¬å½•ä»»åŠ¡"""
        task = await self.get_transcription_status(task_id)
        if not task:
            return False

        if task.status in [TranscriptionStatus.COMPLETED, TranscriptionStatus.FAILED, TranscriptionStatus.CANCELLED]:
            return False

        await self.update_task_progress(
            task_id,
            TranscriptionStatus.CANCELLED,
            task.progress_percentage,
            "Transcription cancelled by user"
        )

        return True

