"""
Podcast Subscription Service - Manages podcast subscriptions.

播客订阅服务 - 管理播客订阅
"""

import logging
from datetime import datetime, timezone
from typing import Any

from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.core.redis import PodcastRedis
from app.domains.podcast.integration.secure_rss_parser import SecureRSSParser
from app.domains.podcast.models import PodcastEpisode
from app.domains.podcast.repositories import PodcastRepository
from app.domains.podcast.schemas import PodcastSubscriptionCreate
from app.domains.subscription.models import Subscription
from app.domains.subscription.repositories import SubscriptionRepository


logger = logging.getLogger(__name__)


class PodcastSubscriptionService:
    """
    Service for managing podcast subscriptions.

    Handles:
    - Adding new subscriptions
    - Listing subscriptions with pagination
    - Refreshing subscriptions
    - Removing subscriptions
    """

    def __init__(self, db: AsyncSession, user_id: int):
        """
        Initialize subscription service.

        Args:
            db: Database session
            user_id: Current user ID
        """
        self.db = db
        self.user_id = user_id
        self.repo = PodcastRepository(db)
        self.redis = PodcastRedis()
        self.parser = SecureRSSParser(user_id)

    async def add_subscription(
        self, feed_url: str
    ) -> tuple[Subscription, list[PodcastEpisode]]:
        """
        Add a new podcast subscription.

        Args:
            feed_url: RSS feed URL

        Returns:
            Tuple of (subscription, new_episodes)

        Raises:
            ValueError: If feed cannot be parsed or limit reached
        """
        # 1. Validate and parse RSS feed
        success, feed, error = await self.parser.fetch_and_parse_feed(feed_url)
        if not success:
            raise ValueError(f"Cannot parse podcast: {error}")

        # 2. Check subscription limit
        existing_subs = await self.repo.get_user_subscriptions(self.user_id)
        if len(existing_subs) >= settings.MAX_PODCAST_SUBSCRIPTIONS:
            raise ValueError(
                f"Maximum subscription limit reached: {settings.MAX_PODCAST_SUBSCRIPTIONS}"
            )

        # 3. Create or update subscription
        metadata = {
            "author": feed.author,
            "language": feed.language,
            "categories": feed.categories,
            "explicit": feed.explicit,
            "image_url": feed.image_url,
            "podcast_type": feed.podcast_type,
            "link": feed.link,
            "total_episodes": len(feed.episodes),
            "platform": feed.platform,
        }

        subscription = await self.repo.create_or_update_subscription(
            self.user_id,
            feed_url,
            feed.title,
            feed.description,
            None,  # custom_name
            metadata=metadata,
        )

        # 4. Save episodes in one transaction.
        episodes_payload = [
            self._build_episode_payload(
                episode=episode,
                feed_title=feed.title,
                extra_metadata=None,
            )
            for episode in feed.episodes
        ]
        _, new_episodes = await self.repo.create_or_update_episodes_batch(
            subscription_id=subscription.id,
            episodes_data=episodes_payload,
        )

        try:
            await self.redis.invalidate_subscription_list(self.user_id)
        except Exception:
            logger.warning(
                "Failed to invalidate subscription list cache after add: user_id=%s",
                self.user_id,
            )

        logger.info(
            f"User {self.user_id} added podcast: {feed.title}, {len(new_episodes)} new episodes"
        )
        return subscription, new_episodes

    async def add_subscriptions_batch(
        self, subscriptions_data: list[PodcastSubscriptionCreate]
    ) -> list[dict[str, Any]]:
        """
        Batch add podcast subscriptions.

        Args:
            subscriptions_data: List of subscription data

        Returns:
            List of result dictionaries
        """
        logger.info(
            f"Starting batch subscription addition: {len(subscriptions_data)} items"
        )
        results = []

        for sub_data in subscriptions_data:
            try:
                # Check if already exists
                existing = await self.repo.get_subscription_by_url(
                    self.user_id, sub_data.feed_url
                )
                if existing:
                    results.append(
                        {
                            "source_url": sub_data.feed_url,
                            "status": "skipped",
                            "message": "Subscription already exists",
                        }
                    )
                    continue

                # Add subscription
                subscription, new_episodes = await self.add_subscription(
                    sub_data.feed_url
                )

                results.append(
                    {
                        "source_url": sub_data.feed_url,
                        "status": "success",
                        "id": subscription.id,
                        "title": subscription.title,
                        "new_episodes": len(new_episodes),
                    }
                )

            except Exception as e:
                logger.error(f"Batch add subscription failed {sub_data.feed_url}: {e}")
                try:
                    await self.db.rollback()
                except Exception as rollback_error:
                    logger.warning(f"Session rollback failed: {rollback_error}")

                results.append(
                    {
                        "source_url": sub_data.feed_url,
                        "status": "error",
                        "message": str(e),
                    }
                )

        return results

    async def list_subscriptions(
        self, filters: dict | None = None, page: int = 1, size: int = 20
    ) -> tuple[list[dict], int]:
        """
        List user subscriptions with pagination.

        Args:
            filters: Optional filters
            page: Page number
            size: Items per page

        Returns:
            Tuple of (subscriptions list, total count)
        """
        cache_filters = self._build_subscription_cache_filters(filters)
        try:
            cached = await self.redis.get_subscription_list(
                self.user_id,
                page,
                size,
                filters=cache_filters,
            )
            if (
                isinstance(cached, dict)
                and isinstance(cached.get("subscriptions"), list)
                and isinstance(cached.get("total"), int)
            ):
                return cached["subscriptions"], cached["total"]
        except Exception:
            logger.warning(
                "Redis cache read failed for subscription list, falling back to DB"
            )

        subscriptions, total = await self.repo.get_user_subscriptions_paginated(
            self.user_id, page=page, size=size, filters=filters
        )

        # Batch fetch episode counts and recent episodes
        subscription_ids = [sub.id for sub in subscriptions]
        episode_counts = await self.repo.get_episodes_counts_batch(subscription_ids)
        episodes_batch = await self.repo.get_subscription_episodes_batch(
            subscription_ids,
            limit_per_subscription=settings.PODCAST_RECENT_EPISODES_LIMIT,
        )

        # Batch fetch playback states
        all_episode_ids = []
        for ep_list in episodes_batch.values():
            all_episode_ids.extend([ep.id for ep in ep_list])
        playback_states = await self.repo.get_playback_states_batch(
            self.user_id, all_episode_ids
        )

        # Build response
        results = []
        for sub in subscriptions:
            episodes = episodes_batch.get(sub.id, [])

            # Calculate unplayed count
            unplayed_count = 0
            for ep in episodes:
                playback = playback_states.get(ep.id)
                if (
                    not playback
                    or not playback.current_position
                    or (
                        ep.audio_duration
                        and playback.current_position < ep.audio_duration * 0.9
                    )
                ):
                    unplayed_count += 1

            episode_count = episode_counts.get(sub.id, 0)

            # Extract metadata from config
            config = sub.config or {}
            image_url = config.get("image_url")
            # Fallback to subscription.image_url column if config doesn't have it
            if not image_url:
                image_url = sub.image_url
            author = config.get("author")
            platform = config.get("platform")

            # Debug logging for missing image_url
            if not image_url:
                logger.warning(
                    f"[DEBUG] Subscription {sub.id} ({sub.title}) has no image_url. config keys: {list(config.keys()) if config else 'config is None'}"
                )
            categories = self._normalize_categories(config.get("categories") or [])
            podcast_type = config.get("podcast_type")
            language = config.get("language")
            explicit = config.get("explicit", False)
            link = config.get("link")
            total_episodes_from_config = config.get("total_episodes")

            # Latest episode
            latest_episode_dict = None
            if episodes:
                latest = episodes[0]
                latest_episode_dict = {
                    "id": latest.id,
                    "title": latest.title,
                    "audio_url": latest.audio_url,
                    "duration": latest.audio_duration,
                    "published_at": latest.published_at,
                    "ai_summary": latest.ai_summary,
                    "status": latest.status,
                }

            results.append(
                {
                    "id": sub.id,
                    "user_id": self.user_id,
                    "title": sub.title,
                    "description": sub.description,
                    "source_url": sub.source_url,
                    "status": sub.status,
                    "last_fetched_at": sub.last_fetched_at,
                    "error_message": sub.error_message,
                    "fetch_interval": sub.fetch_interval,
                    "episode_count": episode_count,
                    "unplayed_count": unplayed_count,
                    "latest_episode": latest_episode_dict,
                    "categories": categories,
                    "image_url": image_url,
                    "author": author,
                    "platform": platform,
                    "podcast_type": podcast_type,
                    "language": language,
                    "explicit": explicit,
                    "link": link,
                    "total_episodes_from_config": total_episodes_from_config,
                    "created_at": sub.created_at,
                    "updated_at": sub.updated_at,
                }
            )

        try:
            await self.redis.set_subscription_list(
                self.user_id,
                page,
                size,
                {"subscriptions": results, "total": total},
                filters=cache_filters,
            )
        except Exception:
            logger.warning("Redis cache write failed for subscription list, skipping")

        return results, total

    async def get_subscription_details(self, subscription_id: int) -> dict | None:
        """
        Get subscription details with episodes.

        Args:
            subscription_id: Subscription ID

        Returns:
            Subscription details dict or None
        """
        sub = await self.repo.get_subscription_by_id(self.user_id, subscription_id)
        if not sub:
            return None

        episodes = await self.repo.get_subscription_episodes(
            subscription_id, limit=settings.PODCAST_EPISODE_BATCH_SIZE
        )
        pending_count = len([e for e in episodes if not e.ai_summary])

        # Extract metadata from config
        config = sub.config or {}
        image_url = config.get("image_url")
        # Fallback to subscription.image_url column if config doesn't have it
        if not image_url:
            image_url = sub.image_url
        author = config.get("author")
        categories = config.get("categories") or []
        podcast_type = config.get("podcast_type")
        language = config.get("language")
        explicit = config.get("explicit", False)
        link = config.get("link")

        return {
            "id": sub.id,
            "title": sub.title,
            "description": sub.description,
            "source_url": sub.source_url,
            "image_url": image_url,
            "author": author,
            "categories": categories,
            "podcast_type": podcast_type,
            "language": language,
            "explicit": explicit,
            "link": link,
            "episode_count": len(episodes),
            "pending_summaries": pending_count,
            "episodes": [
                {
                    "id": ep.id,
                    "title": ep.title,
                    "description": ep.description[:100] + "..."
                    if len(ep.description) > 100
                    else ep.description,
                    "audio_url": ep.audio_url,
                    "duration": ep.audio_duration,
                    "published_at": ep.published_at,
                    "has_summary": ep.ai_summary is not None,
                    "summary": ep.ai_summary[:200] + "..."
                    if ep.ai_summary and len(ep.ai_summary) > 200
                    else ep.ai_summary,
                    "ai_confidence": ep.ai_confidence_score,
                    "play_count": ep.play_count,
                }
                for ep in episodes
            ],
        }

    async def refresh_subscription(self, subscription_id: int) -> list[PodcastEpisode]:
        """
        Refresh podcast subscription to get latest episodes.

        Args:
            subscription_id: Subscription ID

        Returns:
            List of new episodes

        Raises:
            ValueError: If subscription not found or refresh fails
        """
        # Import here to avoid circular dependency

        sub = await self.repo.get_subscription_by_id(self.user_id, subscription_id)
        if not sub:
            raise ValueError("Subscription not found")

        # Parse RSS feed
        success, feed, error = await self.parser.fetch_and_parse_feed(sub.source_url)
        if not success:
            raise ValueError(f"Refresh failed: {error}")

        refreshed_at = datetime.now(timezone.utc).isoformat()
        episodes_payload = [
            self._build_episode_payload(
                episode=episode,
                feed_title=feed.title,
                extra_metadata={"refreshed_at": refreshed_at},
            )
            for episode in feed.episodes
        ]
        _, new_episodes = await self.repo.create_or_update_episodes_batch(
            subscription_id=subscription_id,
            episodes_data=episodes_payload,
        )
        for saved_episode in new_episodes:
            # Manual refresh: NO auto-processing, user requests via frontend
            logger.info(
                "Episode %s discovered via manual refresh, awaiting user request",
                saved_episode.id,
            )

        # Update subscription metadata (including image_url) from feed
        # This ensures the subscription has correct metadata even on first refresh
        metadata = {
            "author": feed.author,
            "language": feed.language,
            "categories": feed.categories,
            "explicit": feed.explicit,
            "image_url": feed.image_url,
            "podcast_type": feed.podcast_type,
            "link": feed.link,
            "total_episodes": len(feed.episodes),
            "platform": feed.platform,
        }

        # Only update metadata if the feed provided valid data
        if feed.image_url or feed.author or feed.categories:
            await self.repo.update_subscription_metadata(subscription_id, metadata)
            logger.debug(
                f"Updated subscription {subscription_id} metadata: image_url={feed.image_url}"
            )

        # Update last fetch time
        await self.repo.update_subscription_fetch_time(
            subscription_id, feed.last_fetched
        )

        # Invalidate episode list cache since we've added new episodes
        await self.redis.invalidate_episode_list(subscription_id)
        await self.redis.invalidate_subscription_list(self.user_id)

        if len(new_episodes) > 0:
            logger.info(
                f"User {self.user_id} refreshed subscription: {sub.title}, found {len(new_episodes)} new episodes"
            )

        return new_episodes

    async def reparse_subscription(
        self, subscription_id: int, force_all: bool = False
    ) -> dict:
        """
        Re-parse all episodes for a subscription.

        Args:
            subscription_id: Subscription ID
            force_all: Force re-parse all episodes (default: only missing)

        Returns:
            Dict with parsing statistics
        """
        sub = await self.repo.get_subscription_by_id(self.user_id, subscription_id)
        if not sub:
            raise ValueError("Subscription not found")

        logger.info(
            f"User {self.user_id} starting re-parse of subscription: {sub.title}"
        )

        # Parse RSS feed
        success, feed, error = await self.parser.fetch_and_parse_feed(sub.source_url)
        if not success:
            raise ValueError(f"Re-parse failed: {error}")

        # Get existing episode links
        existing_item_links = set()
        if not force_all:
            existing_episodes = await self.repo.get_subscription_episodes(
                subscription_id, limit=None
            )
            existing_item_links = {
                ep.item_link for ep in existing_episodes if ep.item_link
            }

        reparsed_at = datetime.now(timezone.utc).isoformat()
        episodes_to_process = [
            episode
            for episode in feed.episodes
            if force_all or episode.link not in existing_item_links
        ]
        episodes_payload = [
            self._build_episode_payload(
                episode=episode,
                feed_title=feed.title,
                extra_metadata={
                    "reparsed_at": reparsed_at,
                    "item_link": episode.link,
                },
            )
            for episode in episodes_to_process
        ]
        (
            processed_episodes,
            new_episode_rows,
        ) = await self.repo.create_or_update_episodes_batch(
            subscription_id=subscription_id,
            episodes_data=episodes_payload,
        )
        processed = len(processed_episodes)
        new_episodes = len(new_episode_rows)
        updated_episodes = processed - new_episodes
        failed = 0

        # Update subscription metadata
        metadata = {
            "author": feed.author,
            "language": feed.language,
            "categories": feed.categories,
            "explicit": feed.explicit,
            "image_url": feed.image_url,
            "podcast_type": feed.podcast_type,
            "link": feed.link,
            "total_episodes": len(feed.episodes),
            "platform": feed.platform,
            "reparsed_at": reparsed_at,
        }

        await self.repo.update_subscription_metadata(subscription_id, metadata)
        await self.repo.update_subscription_fetch_time(
            subscription_id, feed.last_fetched
        )

        # Invalidate episode list cache since we've updated episodes
        await self.redis.invalidate_episode_list(subscription_id)
        await self.redis.invalidate_subscription_list(self.user_id)

        result = {
            "subscription_id": subscription_id,
            "subscription_title": sub.title,
            "total_episodes_in_feed": len(feed.episodes),
            "processed": processed,
            "new_episodes": new_episodes,
            "updated_episodes": updated_episodes,
            "failed": failed,
            "message": f"Re-parse completed: {processed} processed, {new_episodes} new, {updated_episodes} updated, {failed} failed",
        }

        logger.info(f"User {self.user_id} re-parse completed: {result}")
        return result

    async def remove_subscription(self, subscription_id: int) -> bool:
        """
        Unsubscribe current user from a subscription.

        If this was the last subscriber, the shared subscription source
        and related data are deleted by cascade.

        Args:
            subscription_id: Subscription ID

        Returns:
            True if unsubscribed successfully
        """
        try:
            sub = await self._validate_and_get_subscription(subscription_id)
            if not sub:
                return False

            removed = await SubscriptionRepository(self.db).delete_subscription(
                user_id=self.user_id,
                sub_id=subscription_id,
            )
            if not removed:
                return False

            await self.redis.invalidate_episode_list(subscription_id)
            await self.redis.invalidate_subscription_list(self.user_id)
            logger.info(
                f"User {self.user_id} unsubscribed from subscription {subscription_id}"
            )
            return True

        except Exception as e:
            logger.error(f"Failed to remove subscription {subscription_id}: {e}")
            raise

    async def remove_subscriptions_bulk(
        self, subscription_ids: list[int]
    ) -> dict[str, Any]:
        """
        Bulk remove subscriptions.

        Args:
            subscription_ids: List of subscription IDs

        Returns:
            Dict with operation results
        """
        success_count = 0
        failed_count = 0
        errors: list[dict[str, Any]] = []
        deleted_subscription_ids: list[int] = []

        for subscription_id in subscription_ids:
            try:
                removed = await self.remove_subscription(subscription_id)
                if not removed:
                    errors.append(
                        {
                            "subscription_id": subscription_id,
                            "error": f"Subscription {subscription_id} not found or access denied",
                        }
                    )
                    failed_count += 1
                    continue

                success_count += 1
                deleted_subscription_ids.append(subscription_id)
                logger.info(
                    f"User {self.user_id} bulk removed subscription {subscription_id} successfully"
                )

            except Exception as e:
                logger.error(f"Bulk remove subscription {subscription_id} failed: {e}")
                errors.append({"subscription_id": subscription_id, "error": str(e)})
                failed_count += 1

        return {
            "success_count": success_count,
            "failed_count": failed_count,
            "errors": errors,
            "deleted_subscription_ids": deleted_subscription_ids,
        }

    # === Private helper methods ===

    def _normalize_categories(self, raw_categories: list) -> list[dict[str, str]]:
        """Normalize categories to list of dicts."""
        categories = []
        for cat in raw_categories:
            if isinstance(cat, str):
                categories.append({"name": cat})
            elif isinstance(cat, dict):
                categories.append(cat)
            else:
                categories.append({"name": str(cat)})
        return categories

    @staticmethod
    def _build_subscription_cache_filters(filters: Any) -> dict[str, Any]:
        """Build a compact, deterministic filter payload for cache keys."""
        if not filters:
            return {}
        return {
            "category_id": getattr(filters, "category_id", None),
            "status": getattr(filters, "status", None),
        }

    @staticmethod
    def _build_episode_payload(
        *,
        episode: Any,
        feed_title: str,
        extra_metadata: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        metadata = {"feed_title": feed_title}
        if extra_metadata:
            metadata.update(extra_metadata)
        return {
            "title": episode.title,
            "description": episode.description,
            "audio_url": episode.audio_url,
            "published_at": episode.published_at,
            "audio_duration": episode.duration,
            "transcript_url": episode.transcript_url,
            "item_link": episode.link,
            "metadata": metadata,
        }

    async def _validate_and_get_subscription(
        self, subscription_id: int, check_source_type: bool = False
    ) -> Subscription | None:
        """Validate subscription exists and belongs to user."""
        from sqlalchemy import and_, select

        from app.domains.subscription.models import UserSubscription

        stmt = (
            select(Subscription)
            .join(UserSubscription, UserSubscription.subscription_id == Subscription.id)
            .where(
                and_(
                    Subscription.id == subscription_id,
                    UserSubscription.user_id == self.user_id,
                    UserSubscription.is_archived.is_(False),
                )
            )
        )

        if check_source_type:
            stmt = stmt.where(Subscription.source_type == "podcast-rss")

        result = await self.db.execute(stmt)
        return result.scalar_one_or_none()
