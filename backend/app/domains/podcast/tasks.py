"""
æ’­å®¢åå°ä»»åŠ¡ - Celery Tasks
å¤„ç†RSS Feedè‡ªåŠ¨æŠ“å–ã€éŸ³é¢‘è½¬å½•ã€AIæ‘˜è¦ç”Ÿæˆç­‰åå°ä»»åŠ¡
"""

import logging
from typing import List, Optional
from datetime import datetime, timedelta
from celery import Celery
from celery.schedules import crontab
from celery.signals import after_setup_logger
from sqlalchemy import select, delete

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
from app.core.logging_config import setup_logging_from_env
setup_logging_from_env()

from app.core.config import settings
from app.domains.podcast.services import PodcastService
from app.domains.podcast.repositories import PodcastRepository
from app.core.database import get_db_session
from app.integration.podcast.secure_rss_parser import SecureRSSParser
from app.domains.podcast.transcription_manager import DatabaseBackedTranscriptionService
from app.domains.podcast.transcription_state import get_transcription_state_manager
from app.core.database import async_session_factory

# Import all models to ensure SQLAlchemy relationships are properly resolved
# This is critical for Celery workers which don't call init_db()
from app.domains.user.models import User, UserSession
from app.domains.subscription.models import (
    Subscription, SubscriptionItem, SubscriptionCategory,
    SubscriptionCategoryMapping, SubscriptionType, SubscriptionStatus,
    UpdateFrequency
)
from app.domains.knowledge.models import (
    KnowledgeBase, Document, DocumentTag, SearchHistory
)
from app.domains.assistant.models import (
    Conversation, Message, PromptTemplate, AssistantTask
)
from app.domains.multimedia.models import MediaFile, ProcessingJob
from app.domains.podcast.models import (
    PodcastEpisode, PodcastPlaybackState, TranscriptionTask,
    TranscriptionStatus
)
from app.domains.ai.models import AIModelConfig

import asyncio

logger = logging.getLogger(__name__)

# åˆ›å»ºCeleryå®ä¾‹
celery_app = Celery(
    "podcast_tasks",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

# Celeryé…ç½®
celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    # ä»»åŠ¡è·¯ç”±
    task_routes={
        "app.domains.podcast.tasks.refresh_all_podcast_feeds": {"queue": "podcast"},
        "app.domains.podcast.tasks.generate_pending_summaries": {"queue": "ai"},
        "app.domains.podcast.tasks.process_audio_transcription": {"queue": "transcription"},
    },
    # ä»»åŠ¡é…ç½®
    task_track_started=True,
    task_time_limit=30 * 60,  # 30åˆ†é’Ÿè¶…æ—¶
    task_soft_time_limit=25 * 60,  # 25åˆ†é’Ÿè½¯è¶…æ—¶
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)


@celery_app.task(bind=True, max_retries=3)
def refresh_all_podcast_feeds(self):
    """
    å®šæ—¶ä»»åŠ¡ï¼šåˆ·æ–°æ‰€æœ‰æ’­å®¢RSS Feed
    æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡ï¼Œæ£€æŸ¥å“ªäº›è®¢é˜…éœ€è¦æ ¹æ®å…¶è°ƒåº¦é…ç½®è¿›è¡Œæ›´æ–°
    
    æ”¯æŒçš„è°ƒåº¦é¢‘ç‡ï¼š
    - HOURLY: æ¯Nå°æ—¶æ›´æ–°ä¸€æ¬¡ï¼ˆä½¿ç”¨fetch_intervalï¼‰
    - DAILY: æ¯å¤©åœ¨æŒ‡å®šæ—¶é—´æ›´æ–°ï¼ˆä½¿ç”¨update_timeï¼‰
    - WEEKLY: æ¯å‘¨æŒ‡å®šæ˜ŸæœŸå’Œæ—¶é—´æ›´æ–°ï¼ˆä½¿ç”¨update_day_of_weekå’Œupdate_timeï¼‰
    """
    logger.info("å¼€å§‹åˆ·æ–°æ‰€æœ‰æ’­å®¢RSS Feed")

    async def _do_refresh():
        async with async_session_factory() as db:
            try:
                # è·å–æ‰€æœ‰éœ€è¦åˆ·æ–°çš„è®¢é˜…
                repo = PodcastRepository(db)

                # è·å–æ‰€æœ‰æ´»è·ƒçš„æ’­å®¢è®¢é˜…
                # from app.domains.subscription.models import Subscription # Redundant local import removed

                # Get active podcast subscriptions that should be updated now
                stmt = select(Subscription).where(
                    Subscription.source_type == "podcast-rss"
                ).where(
                    Subscription.status == "active"
                )

                result = await db.execute(stmt)
                all_subscriptions = list(result.scalars().all())

                # Filter subscriptions that should be updated now based on their schedule
                subscriptions = [sub for sub in all_subscriptions if sub.should_update_now()]

                refreshed_count = 0
                new_episodes_count = 0

                for sub in subscriptions:
                    try:
                        # ä¸ºæ¯ä¸ªè®¢é˜…åˆ›å»ºæœåŠ¡å®ä¾‹
                        service = PodcastService(db, sub.user_id)

                        # åˆ·æ–°è®¢é˜…
                        new_episodes = await service.refresh_subscription(sub.id)

                        refreshed_count += 1
                        new_episodes_count += len(new_episodes)

                        # service.refresh_subscription already logs progress
                        await service.refresh_subscription(sub.id)

                    except Exception as e:
                        logger.error(f"åˆ·æ–°è®¢é˜… {sub.id} å¤±è´¥: {e}")
                        # ç»§ç»­å¤„ç†å…¶ä»–è®¢é˜…
                        continue

                logger.info(f"RSS Feedåˆ·æ–°å®Œæˆ: {refreshed_count} ä¸ªè®¢é˜…, {new_episodes_count} æœŸæ–°èŠ‚ç›®")

                return {
                    "status": "success",
                    "refreshed_subscriptions": refreshed_count,
                    "new_episodes": new_episodes_count,
                    "processed_at": datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"åˆ·æ–°RSS Feedå¤±è´¥: {e}")
                raise

    try:
        # Run async code in sync Celery worker
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(_do_refresh())
        finally:
            loop.close()

        return result

    except Exception as e:
        logger.error(f"åˆ·æ–°RSS Feedå¤±è´¥: {e}")

        # é‡è¯•é€»è¾‘
        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (2 ** self.request.retries))

        raise


@celery_app.task(bind=True, max_retries=3)
def generate_pending_summaries(self):
    """
    å®šæ—¶ä»»åŠ¡ï¼šç”Ÿæˆå¾…å¤„ç†çš„AIæ‘˜è¦
    æ¯30åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    """
    logger.info("å¼€å§‹ç”Ÿæˆå¾…å¤„ç†çš„AIæ‘˜è¦")

    async def _do_generate():
        async with async_session_factory() as db:
            try:
                repo = PodcastRepository(db)

                # è·å–æ‰€æœ‰å¾…æ€»ç»“çš„å•é›†
                pending_episodes = await repo.get_unsummarized_episodes()

                processed_count = 0
                failed_count = 0

                for episode in pending_episodes:
                    try:
                        # è·å–è®¢é˜…ä¿¡æ¯ä»¥è·å–user_id
                        from app.domains.subscription.models import Subscription
                        stmt = select(Subscription).where(Subscription.id == episode.subscription_id)
                        result = await db.execute(stmt)
                        subscription = result.scalar_one_or_none()

                        if not subscription:
                            logger.error(f"æ‰¾ä¸åˆ°è®¢é˜… {episode.subscription_id}")
                            continue

                        # åˆ›å»ºæœåŠ¡å®ä¾‹
                        service = PodcastService(db, subscription.user_id)

                        # ç”Ÿæˆæ‘˜è¦
                        summary = await service._generate_summary(episode)

                        processed_count += 1
                        logger.info(f"ç”Ÿæˆæ‘˜è¦æˆåŠŸ: {episode.title}")

                    except Exception as e:
                        failed_count += 1
                        logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥ {episode.id}: {e}")
                        # æ ‡è®°ä¸ºå¤±è´¥
                        await repo.mark_summary_failed(episode.id, str(e))
                        continue

                logger.info(f"AIæ‘˜è¦ç”Ÿæˆå®Œæˆ: {processed_count} æˆåŠŸ, {failed_count} å¤±è´¥")

                return {
                    "status": "success",
                    "processed": processed_count,
                    "failed": failed_count,
                    "processed_at": datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"ç”ŸæˆAIæ‘˜è¦å¤±è´¥: {e}")
                raise

    try:
        # Run async code in sync Celery worker
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(_do_generate())
        finally:
            loop.close()

        return result

    except Exception as e:
        logger.error(f"ç”ŸæˆAIæ‘˜è¦å¤±è´¥: {e}")

        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (2 ** self.request.retries))

        raise


@celery_app.task(bind=True, max_retries=3)
def process_audio_transcription(self, task_id: int, config_db_id: Optional[int] = None):
    """
    å¤„ç†éŸ³é¢‘è½¬å½•ä»»åŠ¡
    ä½¿ç”¨å¤–éƒ¨è½¬å½•æœåŠ¡ï¼ˆå¦‚OpenAI Whisperï¼‰è½¬å½•éŸ³é¢‘

    é›†æˆRedisçŠ¶æ€ç®¡ç†:
    - ä»»åŠ¡å¯åŠ¨æ—¶éªŒè¯å¹¶æ›´æ–°é”
    - æ‰§è¡Œè¿‡ç¨‹ä¸­æ›´æ–°Redisç¼“å­˜è¿›åº¦
    - å®Œæˆæˆ–å¤±è´¥æ—¶æ¸…ç†RedisçŠ¶æ€
    """
    logger.info(f"ğŸ¬ [CELERY] å¼€å§‹å¤„ç†éŸ³é¢‘è½¬å½•ä»»åŠ¡: task_id={task_id}, config_id={config_db_id}")

    async def _do_transcription():
        # åˆ›å»ºæ–°çš„æ•°æ®åº“å¼•æ“ï¼ˆé¿å…forkè¿›ç¨‹åäº‹ä»¶å¾ªç¯å†²çªï¼‰
        from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession
        from app.core.config import settings

        # ä¸ºCelery workeråˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“å¼•æ“ï¼ˆä½¿ç”¨NullPoolé¿å…forkåè¿æ¥æ± é—®é¢˜ï¼‰
        worker_engine = create_async_engine(
            settings.DATABASE_URL,
            pool_pre_ping=True,
            pool_size=5,
            max_overflow=10,
            pool_recycle=3600,
            connect_args={
                "server_settings": {
                    "application_name": "celery-worker",
                    "client_encoding": "utf8"
                },
                "timeout": settings.DATABASE_CONNECT_TIMEOUT
            }
        )

        # åˆ›å»ºworkerä¸“ç”¨çš„session factory
        worker_session_factory = async_sessionmaker(
            worker_engine,
            class_=AsyncSession,
            expire_on_commit=False
        )

        try:
            async with worker_session_factory() as session:
                state_manager = await get_transcription_state_manager()

                try:
                    # Get task info to verify episode_id
                    from app.domains.podcast.models import TranscriptionTask

                    stmt = select(TranscriptionTask).where(TranscriptionTask.id == task_id)
                    result = await session.execute(stmt)
                    task = result.scalar_one_or_none()

                    if not task:
                        logger.error(f"âŒ [CELERY] Transcription task {task_id} not found")
                        return

                    episode_id = task.episode_id

                    # Try to acquire lock - only one worker should execute this task
                    lock_acquired = await state_manager.acquire_task_lock(episode_id, task_id, expire_seconds=3600)
                    if not lock_acquired:
                        # Another worker already owns the lock
                        locked_task_id = await state_manager.is_episode_locked(episode_id)
                        logger.info(f"ğŸ”„ [CELERY] Task {task_id} skipping execution - episode {episode_id} already locked by task {locked_task_id}")
                        return

                    logger.info(f"ğŸ”’ [CELERY] Task {task_id} acquired lock for episode {episode_id}")

                    try:
                        # Update Redis initial progress
                        await state_manager.set_task_progress(
                            task_id,
                            "pending",
                            0,
                            "Worker starting transcription process..."
                        )

                        # Execute transcription
                        service = DatabaseBackedTranscriptionService(session)

                        # Patch the service to update Redis progress during execution
                        original_update = service._update_task_progress_with_session

                        async def redis_update_progress(session, task_id, status, progress, message, error_message=None):
                            # Call original DB update
                            await original_update(session, task_id, status, progress, message, error_message)
                            # Also update Redis cache
                            await state_manager.set_task_progress(task_id, status.value if hasattr(status, 'value') else status, progress, message)

                        # Monkey-patch the progress update method
                        service._update_task_progress_with_session = redis_update_progress

                        # Execute the actual transcription
                        await service.execute_transcription_task(task_id, session, config_db_id)

                        # Clear Redis state on success
                        await state_manager.clear_task_state(task_id, episode_id)

                        logger.info(f"âœ… [CELERY] Transcription task {task_id} completed successfully")

                    except Exception as e:
                        logger.error(f"âŒ [CELERY] è½¬å½•ä»»åŠ¡æ‰§è¡Œå‡ºé”™ {task_id}: {e}")
                        import traceback
                        logger.error(traceback.format_exc())

                        # Mark as failed in Redis
                        from app.domains.podcast.models import TranscriptionTask
                        result = await session.execute(stmt)
                        task = result.scalar_one_or_none()

                        if task:
                            await state_manager.fail_task_state(task_id, task.episode_id, str(e))

                        raise
                    finally:
                        # Always release the lock
                        await state_manager.release_task_lock(episode_id, task_id)
                        logger.info(f"ğŸ”“ [CELERY] Task {task_id} released lock for episode {episode_id}")

                except Exception as e:
                    logger.error(f"âŒ [CELERY] Unexpected error during transcription: {e}")
                    import traceback
                    logger.error(traceback.format_exc())

        finally:
            # ç¡®ä¿å…³é—­workerå¼•æ“
            await worker_engine.dispose()

    try:
        # Run async code in sync Celery worker
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(_do_transcription())
        finally:
            loop.close()

        return {
            "status": "success",
            "task_id": task_id,
            "processed_at": datetime.utcnow().isoformat()
        }

    except Exception as e:
        logger.error(f"âŒ [CELERY] éŸ³é¢‘è½¬å½•ä»»åŠ¡å¤±è´¥ {task_id}: {e}")

        if self.request.retries < self.max_retries:
            logger.info(f"ğŸ”„ [CELERY] Retrying task {task_id} (attempt {self.request.retries + 1}/{self.max_retries})")
            raise self.retry(countdown=60 * (2 ** self.request.retries))

        # Max retries exceeded - mark as permanently failed
        logger.error(f"âŒ [CELERY] Task {task_id} failed after {self.max_retries} retries")

        # Try to update Redis state one more time
        async def _mark_failed():
            state_manager = await get_transcription_state_manager()
            async with async_session_factory() as session:
                from app.domains.podcast.models import TranscriptionTask

                stmt = select(TranscriptionTask).where(TranscriptionTask.id == task_id)
                result = await session.execute(stmt)
                task = result.scalar_one_or_none()

                if task:
                    await state_manager.fail_task_state(task_id, task.episode_id, f"Failed after {self.max_retries} retries: {str(e)}")

        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(_mark_failed())
            finally:
                loop.close()
        except Exception as cleanup_error:
            logger.error(f"Failed to mark task as failed in Redis: {cleanup_error}")

        raise


@celery_app.task
def generate_summary_for_episode(episode_id: int, user_id: int):
    """
    ä¸ºæŒ‡å®šå•é›†ç”ŸæˆAIæ‘˜è¦
    å¯ä»¥è¢«å…¶ä»–ä»»åŠ¡è°ƒç”¨
    """
    logger.info(f"å¼€å§‹ç”Ÿæˆå•é›†æ‘˜è¦: episode {episode_id}, user {user_id}")

    async def _do_generate():
        async with async_session_factory() as db:
            try:
                service = PodcastService(db, user_id)

                # ç”Ÿæˆæ‘˜è¦
                summary = await service._generate_summary_task(
                    await service.repo.get_episode_by_id(episode_id)
                )

                return {
                    "status": "success",
                    "episode_id": episode_id,
                    "summary": summary,
                    "processed_at": datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"ç”Ÿæˆå•é›†æ‘˜è¦å¤±è´¥ {episode_id}: {e}")
                raise

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(_do_generate())
        finally:
            loop.close()

        return result

    except Exception as e:
        logger.error(f"ç”Ÿæˆå•é›†æ‘˜è¦å¤±è´¥ {episode_id}: {e}")
        raise


@celery_app.task
def cleanup_old_playback_states():
    """
    æ¸…ç†ä»»åŠ¡ï¼šæ¸…ç†æ—§çš„æ’­æ”¾çŠ¶æ€è®°å½•
    æ¯å¤©æ‰§è¡Œä¸€æ¬¡ï¼Œä¿ç•™æœ€è¿‘90å¤©çš„è®°å½•
    """
    logger.info("å¼€å§‹æ¸…ç†æ—§çš„æ’­æ”¾çŠ¶æ€è®°å½•")

    async def _do_cleanup():
        async with async_session_factory() as db:
            try:
                # åˆ é™¤90å¤©å‰çš„æ’­æ”¾è®°å½•
                cutoff_date = datetime.utcnow() - timedelta(days=90)

                from app.domains.podcast.models import PodcastPlaybackState

                stmt = delete(PodcastPlaybackState).where(
                    PodcastPlaybackState.last_updated_at < cutoff_date
                )

                result = await db.execute(stmt)
                deleted_count = result.rowcount
                await db.commit()

                logger.info(f"æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count} æ¡æ—§è®°å½•")

                return {
                    "status": "success",
                    "deleted_count": deleted_count,
                    "processed_at": datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"æ¸…ç†æ—§è®°å½•å¤±è´¥: {e}")
                raise

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(_do_cleanup())
        finally:
            loop.close()

        return result

    except Exception as e:
        logger.error(f"æ¸…ç†æ—§è®°å½•å¤±è´¥: {e}")
        raise


@celery_app.task
def generate_podcast_recommendations():
    """
    æ¨èä»»åŠ¡ï¼šä¸ºæ‰€æœ‰ç”¨æˆ·ç”Ÿæˆæ’­å®¢æ¨è
    æ¯å¤©æ‰§è¡Œä¸€æ¬¡
    """
    logger.info("å¼€å§‹ç”Ÿæˆæ’­å®¢æ¨è")

    async def _do_generate():
        async with async_session_factory() as db:
            try:
                # è·å–æ‰€æœ‰ç”¨æˆ·
                from app.domains.user.models import User

                stmt = select(User).where(User.is_active == True)
                result = await db.execute(stmt)
                users = list(result.scalars().all())

                recommendations_generated = 0

                for user in users:
                    try:
                        service = PodcastService(db, user.id)
                        recommendations = await service.get_recommendations(limit=20)

                        # TODO: å°†æ¨èç»“æœä¿å­˜åˆ°æ¨èè¡¨æˆ–ç¼“å­˜ä¸­

                        recommendations_generated += len(recommendations)

                    except Exception as e:
                        logger.error(f"ä¸ºç”¨æˆ· {user.id} ç”Ÿæˆæ¨èå¤±è´¥: {e}")
                        continue

                logger.info(f"æ¨èç”Ÿæˆå®Œæˆ: {recommendations_generated} æ¡æ¨è")

                return {
                    "status": "success",
                    "recommendations_generated": recommendations_generated,
                    "processed_at": datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"ç”Ÿæˆæ¨èå¤±è´¥: {e}")
                raise

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(_do_generate())
        finally:
            loop.close()

        return result

    except Exception as e:
        logger.error(f"ç”Ÿæˆæ¨èå¤±è´¥: {e}")
        raise


@celery_app.task
def cleanup_old_transcription_temp_files(days: int = 7):
    """
    æ¸…ç†ä»»åŠ¡ï¼šæ¸…ç†æ—§çš„è½¬å½•ä¸´æ—¶æ–‡ä»¶
    æ¯å¤©æ‰§è¡Œä¸€æ¬¡ï¼Œæ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„å¤±è´¥/å·²å–æ¶ˆä»»åŠ¡çš„ä¸´æ—¶æ–‡ä»¶

    Args:
        days: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤7å¤©
    """
    logger.info(f"å¼€å§‹æ¸…ç†æ—§è½¬å½•ä¸´æ—¶æ–‡ä»¶ (ä¿ç•™ {days} å¤©)")

    async def _do_cleanup():
        async with async_session_factory() as db:
            try:
                from app.domains.podcast.transcription_manager import DatabaseBackedTranscriptionService

                service = DatabaseBackedTranscriptionService(db)
                result = await service.cleanup_old_temp_files(days=days)

                return {
                    "status": "success",
                    **result,
                    "processed_at": datetime.utcnow().isoformat()
                }

            except Exception as e:
                logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                raise

    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(_do_cleanup())
        finally:
            loop.close()

        return result

    except Exception as e:
        logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        raise


# === è¾…åŠ©å‡½æ•° ===

def _simulate_transcription(title: str, description: str) -> str:
    """
    æ¨¡æ‹ŸéŸ³é¢‘è½¬å½•
    å®é™…åº”ç”¨ä¸­åº”è¯¥è°ƒç”¨çœŸå®çš„è½¬å½•æœåŠ¡
    """
    # åŸºäºæ ‡é¢˜å’Œæè¿°ç”Ÿæˆæ¨¡æ‹Ÿè½¬å½•æ–‡æœ¬
    mock_transcription = f"""
    æ’­å®¢æ ‡é¢˜: {title}

    è½¬å½•å†…å®¹:
    æ¬¢è¿æ”¶å¬æœ¬æœŸèŠ‚ç›®ã€‚ä»Šå¤©æˆ‘ä»¬å°†è®¨è®ºå…³äº {title} çš„è¯é¢˜ã€‚

    {description[:500] if description else "æœ¬æœŸèŠ‚ç›®å†…å®¹ç²¾å½©ï¼Œè¯·æ”¶å¬å®Œæ•´éŸ³é¢‘ã€‚"}

    æ„Ÿè°¢æ‚¨çš„æ”¶å¬ï¼Œæˆ‘ä»¬ä¸‹æœŸå†è§ã€‚

    ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿè½¬å½•æ–‡æœ¬ï¼Œå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨çœŸå®çš„éŸ³é¢‘è½¬å½•æœåŠ¡ï¼‰
    """

    return mock_transcription.strip()


# === Celery Beat å®šæ—¶ä»»åŠ¡é…ç½® ===

# é…ç½®å®šæ—¶ä»»åŠ¡
celery_app.conf.beat_schedule = {
    # Check for subscriptions to update every hour (on the hour)
    'refresh-podcast-feeds': {
        'task': 'app.domains.podcast.tasks.refresh_all_podcast_feeds',
        'schedule': crontab(minute=0),  # Top of every hour
        'options': {'queue': 'podcast'}
    },

    # æ¯30åˆ†é’Ÿç”Ÿæˆå¾…å¤„ç†çš„AIæ‘˜è¦
    'generate-pending-summaries': {
        'task': 'app.domains.podcast.tasks.generate_pending_summaries',
        'schedule': 1800.0,  # 30åˆ†é’Ÿ
        'options': {'queue': 'ai'}
    },

    # æ¯å¤©å‡Œæ™¨2ç‚¹æ¸…ç†æ—§è®°å½•
    'cleanup-old-records': {
        'task': 'app.domains.podcast.tasks.cleanup_old_playback_states',
        'schedule': 86400.0,  # 24å°æ—¶
        'options': {'queue': 'cleanup'}
    },

    # æ¯å¤©å‡Œæ™¨4ç‚¹æ¸…ç†æ—§çš„è½¬å½•ä¸´æ—¶æ–‡ä»¶ï¼ˆåœ¨æ¸…ç†æ—§è®°å½•å’Œç”Ÿæˆæ¨èä¹‹åï¼‰
    'cleanup-transcription-temp-files': {
        'task': 'app.domains.podcast.tasks.cleanup_old_transcription_temp_files',
        'schedule': 86400.0,  # 24å°æ—¶
        'options': {'queue': 'cleanup'}
    },

    # æ¯å¤©å‡Œæ™¨3ç‚¹ç”Ÿæˆæ¨è
    'generate-recommendations': {
        'task': 'app.domains.podcast.tasks.generate_podcast_recommendations',
        'schedule': 86400.0,  # 24å°æ—¶
        'options': {'queue': 'recommendation'}
    },
}