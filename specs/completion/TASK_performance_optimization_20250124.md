# Performance Optimization - Task Tracking / æ€§èƒ½ä¼˜åŒ– - ä»»åŠ¡è·Ÿè¸ª

## Project Overview / é¡¹ç›®æ¦‚è§ˆ

- **Requirement ID / éœ€æ±‚ID**: REQ-20250124-001
- **Project Name / é¡¹ç›®åç§°**: Performance Optimization / æ€§èƒ½ä¼˜åŒ–
- **Created Date / åˆ›å»ºæ—¥æœŸ**: 2025-01-24
- **Target Release / ç›®æ ‡å‘å¸ƒ**: 2025-02-07
- **Status / çŠ¶æ€**: ğŸ”„ In Progress / è¿›è¡Œä¸­

## Progress Summary / è¿›åº¦æ‘˜è¦

| Category / ç±»åˆ« | Completed / å·²å®Œæˆ | In Progress / è¿›è¡Œä¸­ | Pending / å¾…å¤„ç† | Total / æ€»è®¡ | Progress / è¿›åº¦ |
|----------------|-------------------|-------------------|----------------|-------------|---------------|
| Backend / åç«¯ | 0 | 0 | 5 | 5 | 0% |
| Frontend / å‰ç«¯ | 0 | 0 | 4 | 4 | 0% |
| Testing / æµ‹è¯• | 0 | 0 | 2 | 2 | 0% |
| **Total / æ€»è®¡** | **0** | **0** | **11** | **11** | **0%** |

## Backend Tasks / åç«¯ä»»åŠ¡

### TASK-B-001: Fix N+1 Query in Search Episodes / ä¿®å¤æœç´¢å•é›†çš„ N+1 æŸ¥è¯¢

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Backend Developer

**Priority / ä¼˜å…ˆçº§**: Critical

**Estimated Time / é¢„ä¼°æ—¶é—´**: 2 hours

**File / æ–‡ä»¶**: `backend/app/domains/podcast/services.py:320-387`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¿®å¤ `search_podcasts` æ–¹æ³•ä¸­çš„ N+1 æŸ¥è¯¢é—®é¢˜ï¼ˆLine 338ï¼‰ï¼Œä½¿ç”¨æ‰¹é‡æŸ¥è¯¢æ›¿ä»£å¾ªç¯æŸ¥è¯¢ã€‚

**English**: Fix N+1 query issue in `search_podcasts` method (Line 338), use batch query instead of loop query.

**Current Problem / å½“å‰é—®é¢˜**:
```python
# Line 337-339: N+1 Query
for ep in episodes:
    playback = await self.repo.get_playback_state(self.user_id, ep.id)  # N+1!
```

**Solution / è§£å†³æ–¹æ¡ˆ**:
```python
# Use batch query like in list_episodes (Line 265-267)
episode_ids = [ep.id for ep in episodes]
playback_states = await self.repo.get_playback_states_batch(self.user_id, episode_ids)

for ep in episodes:
    playback = playback_states.get(ep.id)  # O(1) lookup!
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Replace loop with `get_playback_states_batch`
- [ ] Unit test: Mock 20 episodes, verify only 1 playback state query
- [ ] Integration test: Search returns < 500ms
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected improvement: 20 episodes Ã— 50ms = 1s â†’ 1 batch query Ã— 100ms = 0.1s (90% faster)
- Reference implementation: `list_episodes` method (Line 265-267)

---

### TASK-B-002: Optimize User Stats Calculation / ä¼˜åŒ–ç”¨æˆ·ç»Ÿè®¡è®¡ç®—

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Backend Developer

**Priority / ä¼˜å…ˆçº§**: Critical

**Estimated Time / é¢„ä¼°æ—¶é—´**: 4 hours

**File / æ–‡ä»¶**: `backend/app/domains/podcast/services.py:569-614`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¼˜åŒ– `get_user_stats` æ–¹æ³•çš„åµŒå¥—å¾ªç¯æŸ¥è¯¢ï¼Œä½¿ç”¨ SQL èšåˆä¸€æ¬¡æŸ¥è¯¢å®Œæˆã€‚

**English**: Optimize nested loop queries in `get_user_stats` method, use SQL aggregation for single query.

**Current Problem / å½“å‰é—®é¢˜**:
```python
# Line 571-593: Nested loops O(n*m)
for sub in subscriptions:  # n subscriptions
    episodes = await self.repo.get_subscription_episodes(sub.id, limit=None)  # Query 1
    total_episodes += len(episodes)

    for ep in episodes:  # m episodes
        if ep.ai_summary:
            summaries_generated += 1
        playback = await self.repo.get_playback_state(self.user_id, ep.id)  # Query 2! N+1
        if playback:
            total_playtime += playback.current_position
```

**Solution / è§£å†³æ–¹æ¡ˆ**:
```python
# Use SQL aggregation with JOINs
SELECT
    COUNT(DISTINCT s.id) as total_subscriptions,
    COUNT(DISTINCT e.id) as total_episodes,
    SUM(CASE WHEN e.ai_summary IS NOT NULL THEN 1 ELSE 0 END) as summaries_generated,
    COALESCE(SUM(ps.current_position), 0) as total_playtime
FROM subscriptions s
LEFT JOIN podcast_episodes e ON e.subscription_id = s.id
LEFT JOIN podcast_playback_states ps ON ps.episode_id = e.id AND ps.user_id = :user_id
WHERE s.user_id = :user_id
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Implement SQL aggregation query (single query)
- [ ] Unit test: 100 subscriptions, verify query count = 1
- [ ] Integration test: Stats endpoint returns < 200ms
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected improvement: 100 subs Ã— 50 eps Ã— 50ms = 250s â†’ 1 query Ã— 500ms = 0.5s (99.8% faster)
- Need to add repository method: `get_user_stats_aggregated(user_id)`

---

### TASK-B-003: Fix Subscription List N+1 Query / ä¿®å¤è®¢é˜…åˆ—è¡¨ N+1 æŸ¥è¯¢

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Backend Developer

**Priority / ä¼˜å…ˆçº§**: High

**Estimated Time / é¢„ä¼°æ—¶é—´**: 2 hours

**File / æ–‡ä»¶**: `backend/app/domains/subscription/services.py:52-60`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¿®å¤ `list_subscriptions` ä¸­çš„ N+1 æŸ¥è¯¢ï¼Œä½¿ç”¨ LEFT JOIN COUNT èšåˆä¸€æ¬¡æŸ¥è¯¢ã€‚

**English**: Fix N+1 query in `list_subscriptions`, use LEFT JOIN COUNT aggregation for single query.

**Current Problem / å½“å‰é—®é¢˜**:
```python
# Line 52-60: N+1 Query
for sub in items:
    # Query for each subscription!
    count_query = select(func.count()).select_from(
        select(SubscriptionItem)
        .where(SubscriptionItem.subscription_id == sub.id)
        .subquery()
    )
    item_count = await self.db.scalar(count_query) or 0
```

**Solution / è§£å†³æ–¹æ¡ˆ**:
```python
# Single query with LEFT JOIN COUNT
query = (
    select(Subscription, func.count(SubscriptionItem.id).label('item_count'))
    .outerjoin(SubscriptionItem, SubscriptionItem.subscription_id == Subscription.id)
    .where(Subscription.user_id == self.user_id)
    .group_by(Subscription.id)
)

result = await self.db.execute(query)
for sub, item_count in result:
    # No additional query needed!
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Use LEFT JOIN COUNT aggregation
- [ ] Unit test: 20 subscriptions, verify query count = 1
- [ ] Integration test: Subscription list returns < 500ms
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected improvement: 20 subs Ã— 50ms = 1s â†’ 1 query Ã— 100ms = 0.1s (90% faster)
- Need to update `SubscriptionRepository.get_user_subscriptions` method

---

### TASK-B-004: Implement Redis Caching Layer / å®ç° Redis ç¼“å­˜å±‚

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Backend Developer

**Priority / ä¼˜å…ˆçº§**: High

**Estimated Time / é¢„ä¼°æ—¶é—´**: 8 hours

**File / æ–‡ä»¶**:
- `backend/app/core/redis.py` (extend)
- New: `backend/app/domains/podcast/cache.py`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¸ºå¸¸ç”¨æŸ¥è¯¢å®ç° Redis ç¼“å­˜å±‚ï¼ŒåŒ…æ‹¬ç¼“å­˜è£…é¥°å™¨å’Œå¤±æ•ˆç­–ç•¥ã€‚

**English**: Implement Redis caching layer for common queries, including cache decorators and invalidation strategy.

**Implementation Details / å®ç°ç»†èŠ‚**:

1. **Cache Decorator / ç¼“å­˜è£…é¥°å™¨**:
```python
# backend/app/core/cache.py
from functools import wraps
from app.core.redis import get_redis

def cache_result(ttl: int = 300, key_prefix: str = ""):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            redis = get_redis()
            cache_key = f"{key_prefix}:{args}:{kwargs}"

            # Try cache
            cached = await redis.get(cache_key)
            if cached:
                return json.loads(cached)

            # Cache miss, execute function
            result = await func(*args, **kwargs)

            # Write to cache
            await redis.setex(cache_key, ttl, json.dumps(result))
            return result
        return wrapper
    return decorator
```

2. **Cache Strategy / ç¼“å­˜ç­–ç•¥**:

| Cache Key / ç¼“å­˜é”® | TTL | Invalidated By / å¤±æ•ˆæ¡ä»¶ |
|-------------------|-----|-------------------------|
| `subscriptions:user:{user_id}:page:{page}` | 5 min | Subscription create/update/delete |
| `episodes:sub:{sub_id}:page:{page}` | 5 min | Episode update, new episodes fetched |
| `playback_states:user:{user_id}:episodes:{ep_ids}` | 10 min | Playback progress update |
| `stats:user:{user_id}` | 15 min | Episode played, subscription added/removed |
| `search:{user_id}:{query_hash}` | 30 min | Episode update, new episodes fetched |

3. **Cache Invalidation / ç¼“å­˜å¤±æ•ˆ**:
```python
async def invalidate_subscription_cache(user_id: int, subscription_id: int):
    redis = get_redis()
    pattern = f"subscriptions:user:{user_id}:*"
    await redis.delete_pattern(pattern)

async def invalidate_episode_cache(subscription_id: int):
    redis = get_redis()
    pattern = f"episodes:sub:{subscription_id}:*"
    await redis.delete_pattern(pattern)
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Implement cache decorators for common queries
- [ ] Cache invalidation on create/update/delete
- [ ] Unit test: Cache hit, cache miss, cache invalidation
- [ ] Integration test: Cache hit rate > 70%
- [ ] Code review approved

**Dependencies / ä¾èµ–**: TASK-B-001, TASK-B-002, TASK-B-003

**Blocked By / è¢«é˜»å¡**: TASK-B-001, TASK-B-002, TASK-B-003

**Notes / å¤‡æ³¨**:
- Use existing Redis infrastructure
- Implement cache warming for frequently accessed data
- Monitor cache memory usage

---

### TASK-B-005: Add Performance Monitoring / æ·»åŠ æ€§èƒ½ç›‘æ§

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Backend Developer

**Priority / ä¼˜å…ˆçº§**: Medium

**Estimated Time / é¢„ä¼°æ—¶é—´**: 6 hours

**File / æ–‡ä»¶**: New `backend/app/core/metrics.py`

**Description / æè¿°**:
**ä¸­æ–‡**: æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œç›‘æ§ï¼ŒåŒ…æ‹¬æŸ¥è¯¢æ—¶é—´ã€ç¼“å­˜å‘½ä¸­ç‡ã€å“åº”æ—¶é—´ç­‰ã€‚

**English**: Add performance metrics collection and monitoring, including query time, cache hit rate, response time, etc.

**Implementation Details / å®ç°ç»†èŠ‚**:

1. **Metrics Collector / æŒ‡æ ‡æ”¶é›†å™¨**:
```python
# backend/app/core/metrics.py
from time import time
from contextlib import asynccontextmanager

class MetricsCollector:
    def __init__(self):
        self.metrics = {}

    @asynccontextmanager
    async def track_query(self, query_name: str):
        start = time()
        try:
            yield
        finally:
            duration = time() - start
            self.record_query(query_name, duration)

    def record_query(self, query_name: str, duration: float):
        if query_name not in self.metrics:
            self.metrics[query_name] = []
        self.metrics[query_name].append(duration)

        # Log slow queries
        if duration > 1.0:
            logger.warning(f"Slow query: {query_name} took {duration:.2f}s")
```

2. **Metrics Endpoint / æŒ‡æ ‡ç«¯ç‚¹**:
```python
# backend/app/api/v1/endpoints/metrics.py
@router.get("/metrics")
async def get_metrics(current_user: User = Depends(get_current_user)):
    metrics_collector = get_metrics_collector()

    return {
        "queries": metrics_collector.metrics,
        "cache_stats": {
            "hit_rate": cache_hit_rate,
            "total_hits": total_hits,
            "total_misses": total_misses,
        },
        "performance": {
            "p50_response_time": p50,
            "p95_response_time": p95,
            "p99_response_time": p99,
        }
    }
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Collect query time, cache hit rate
- [ ] Expose metrics endpoint `/api/v1/metrics`
- [ ] Log slow queries (> 1s)
- [ ] Dashboard for visualization (optional)
- [ ] Code review approved

**Dependencies / ä¾èµ–**: TASK-B-004

**Blocked By / è¢«é˜»å¡**: TASK-B-004

**Notes / å¤‡æ³¨**:
- Consider using Prometheus + Grafana for production monitoring
- Metrics should be tracked per-user for debugging

---

## Frontend Tasks / å‰ç«¯ä»»åŠ¡

### TASK-F-001: Implement Request Cache / å®ç°è¯·æ±‚ç¼“å­˜

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Frontend Developer

**Priority / ä¼˜å…ˆçº§**: High

**Estimated Time / é¢„ä¼°æ—¶é—´**: 4 hours

**File / æ–‡ä»¶**: `frontend/lib/core/network/dio_client.dart`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¸º Dio å®¢æˆ·ç«¯æ·»åŠ è¯·æ±‚ç¼“å­˜æ‹¦æˆªå™¨ï¼Œå‡å°‘é‡å¤çš„ç½‘ç»œè¯·æ±‚ã€‚

**English**: Add request cache interceptor for Dio client to reduce redundant network requests.

**Implementation Details / å®ç°ç»†èŠ‚**:

1. **Add Dependency / æ·»åŠ ä¾èµ–**:
```yaml
# frontend/pubspec.yaml
dependencies:
  dio_cache_interceptor: ^3.4.0
  dio_cache_interceptor_hive_store: ^3.2.1
```

2. **Configure Cache / é…ç½®ç¼“å­˜**:
```dart
// frontend/lib/core/network/dio_client.dart
import 'package:dio_cache_interceptor/dio_cache_interceptor.dart';
import 'package:dio_cache_interceptor_hive_store/dio_cache_interceptor_hive_store.dart';

class DioClient {
  DioClient() {
    // ... existing code ...

    // Add cache interceptor
    final cacheStore = HiveCacheStore('dio_cache');
    final cacheOptions = CacheOptions(
      store: cacheStore,
      policy: CachePolicy.request,
      hitCacheOnErrorExcept: [401, 403],
      maxStale: Duration(minutes: 5),
      priority: CachePriority.high,
      cipher: null,
      keyBuilder: (request) => request.uri.toString(),
    );

    _dio.interceptors.add(DioCacheInterceptor(options: cacheOptions));
  }
}
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Add `dio_cache_interceptor` dependency
- [ ] Configure cache policy (5 min TTL)
- [ ] Widget test: Verify cache hit on second request
- [ ] Manual test: Navigate back to previous page, verify no network request
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected: 70% reduction in navigation requests
- Cache should be cleared on logout

---

### TASK-F-002: Implement Search Debounce / å®ç°æœç´¢é˜²æŠ–

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Frontend Developer

**Priority / ä¼˜å…ˆçº§**: Critical

**Estimated Time / é¢„ä¼°æ—¶é—´**: 2 hours

**File / æ–‡ä»¶**: `frontend/lib/features/podcast/presentation/providers/podcast_providers.dart:707`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¸ºæœç´¢è¾“å…¥æ¡†æ·»åŠ é˜²æŠ–åŠŸèƒ½ï¼Œå‡å°‘ä¸å¿…è¦çš„æœç´¢è¯·æ±‚ã€‚

**English**: Add debounce functionality to search input to reduce unnecessary search requests.

**Current Problem / å½“å‰é—®é¢˜**:
```dart
// Line 725-756: No debounce
Future<void> searchPodcasts({
  required String query,
  // ...
}) async {
  // Fires immediately on every keystroke!
  state = const AsyncValue.loading();
  final response = await _repository.searchPodcasts(/* ... */);
}
```

**Solution / è§£å†³æ–¹æ¡ˆ**:
```dart
// Add Timer for debounce
Timer? _debounceTimer;

Future<void> searchPodcasts({
  required String query,
  // ...
}) async {
  // Cancel previous timer
  _debounceTimer?.cancel();

  // Set new timer (500ms debounce)
  _debounceTimer = Timer(Duration(milliseconds: 500), () async {
    if (query.trim().isEmpty) {
      state = AsyncValue.data(const PodcastEpisodeListResponse(/* ... */));
      return;
    }

    state = const AsyncValue.loading();
    try {
      final response = await _repository.searchPodcasts(/* ... */);
      state = AsyncValue.data(response);
    } catch (error, stackTrace) {
      state = AsyncValue.error(error, stackTrace);
    }
  });
}

@override
void dispose() {
  _debounceTimer?.cancel();
  super.dispose();
}
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Implement 500ms debounce
- [ ] Widget test: Verify only 1 request after rapid typing
- [ ] Manual test: Type quickly, verify only 1 request after stopping
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected: 90% reduction in search API calls
- Show loading indicator only after debounce period

---

### TASK-F-003: Implement Playback Progress Throttle / å®ç°æ’­æ”¾è¿›åº¦èŠ‚æµ

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Frontend Developer

**Priority / ä¼˜å…ˆçº§**: High

**Estimated Time / é¢„ä¼°æ—¶é—´**: 3 hours

**File / æ–‡ä»¶**: `frontend/lib/features/podcast/presentation/providers/podcast_providers.dart:384`

**Description / æè¿°**:
**ä¸­æ–‡**: ä¸ºæ’­æ”¾è¿›åº¦æ›´æ–°æ·»åŠ èŠ‚æµåŠŸèƒ½ï¼Œå‡å°‘æ‹–åŠ¨è¿›åº¦æ¡æ—¶çš„è¯·æ±‚æ¬¡æ•°ã€‚

**English**: Add throttle functionality to playback progress updates to reduce requests when dragging progress bar.

**Current Problem / å½“å‰é—®é¢˜**:
```dart
// Line 384-412: No throttle - fires on every position update
Future<void> _updatePlaybackStateOnServer() async {
  if (_isDisposed) return;
  // Fires tens of times when dragging!
  await _repository.updatePlaybackProgress(/* ... */);
}
```

**Solution / è§£å†³æ–¹æ¡ˆ**:
```dart
// Add throttle variables
Timer? _throttleTimer;
DateTime? _lastUpdate;

Future<void> _updatePlaybackStateOnServer() async {
  if (_isDisposed) return;

  final episode = state.currentEpisode;
  if (episode == null) return;

  final now = DateTime.now();

  // Throttle: Only update every 2 seconds
  if (_lastUpdate != null &&
      now.difference(_lastUpdate!).inSeconds < 2) {
    // Schedule update if not already scheduled
    _throttleTimer?.cancel();
    _throttleTimer = Timer(Duration(seconds: 2), () {
      _updatePlaybackStateOnServer();
    });
    return;
  }

  _lastUpdate = now;
  _throttleTimer?.cancel();

  try {
    await _repository.updatePlaybackProgress(
      episodeId: episode.id,
      position: (state.position / 1000).round(),
      isPlaying: state.isPlaying,
      playbackRate: state.playbackRate,
    );
  } catch (error) {
    logger.AppLogger.debug('âš ï¸ Failed to update playback state: $error');
  }
}

// Call immediately when user releases progress bar
void onDragEnd() {
  _throttleTimer?.cancel();
  _lastUpdate = null;
  _updatePlaybackStateOnServer();  // Immediate update
}
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Implement 2s throttle
- [ ] Immediate update on drag end
- [ ] Widget test: Verify throttled updates
- [ ] Manual test: Drag progress bar, verify max 1 request per 2 seconds
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected: 95% reduction in progress update requests
- Ensure final position is always saved

---

### TASK-F-004: Implement Page State Caching / å®ç°é¡µé¢çŠ¶æ€ç¼“å­˜

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Frontend Developer

**Priority / ä¼˜å…ˆçº§**: Medium

**Estimated Time / é¢„ä¼°æ—¶é—´**: 6 hours

**File / æ–‡ä»¶**: Multiple provider files

**Description / æè¿°**:
**ä¸­æ–‡**: ä½¿ç”¨ Riverpod çš„ `keepAlive()` ä¿æŒé¡µé¢çŠ¶æ€ï¼Œé¿å…é‡å¤åŠ è½½æ•°æ®ã€‚

**English**: Use Riverpod's `keepAlive()` to maintain page state and avoid reloading data.

**Implementation Details / å®ç°ç»†èŠ‚**:

1. **Update Providers / æ›´æ–° Providers**:
```dart
// Before: Recreates state every time
final podcastSubscriptionProvider = AsyncNotifierProvider<PodcastSubscriptionNotifier, PodcastSubscriptionState>(
  PodcastSubscriptionNotifier.new
);

// After: Keeps state alive
final podcastSubscriptionProvider = AsyncNotifierProvider<PodcastSubscriptionNotifier, PodcastSubscriptionState>(
  PodcastSubscriptionNotifier.new,
  // Keep alive when not in use
  keepAlive: true,
);

class PodcastSubscriptionNotifier extends AsyncNotifier<PodcastSubscriptionState> {
  @override
  PodcastSubscriptionState build() {
    ref.onDispose(() {
      // Cleanup resources
    });
    // ...
  }
}
```

2. **Cache Invalidation Strategy / ç¼“å­˜å¤±æ•ˆç­–ç•¥**:
```dart
// Invalidate cache when data changes
Future<void> addSubscription(SubscriptionCreate data) async {
  await _repository.createSubscription(data);

  // Invalidate cache
  ref.invalidate(podcastSubscriptionProvider);
}

// Or use auto-invalidation with Riverpod 2.0
final podcastSubscriptionProvider = AsyncNotifierProvider.autoDispose
  .family<PodcastSubscriptionNotifier, PodcastSubscriptionState, int>(
  PodcastSubscriptionNotifier.new,
);
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Use `keepAlive()` for providers
- [ ] Implement cache invalidation strategy
- [ ] Widget test: Verify data persistence on navigation
- [ ] Manual test: Navigate away and back, verify no loading
- [ ] Code review approved

**Dependencies / ä¾èµ–**: None

**Blocked By / è¢«é˜»å¡**: None

**Notes / å¤‡æ³¨**:
- Expected: 60% reduction in page load requests
- Balance between cache hits and freshness

---

## Testing Tasks / æµ‹è¯•ä»»åŠ¡

### TASK-T-001: Performance Testing Plan / æ€§èƒ½æµ‹è¯•è®¡åˆ’

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Test Engineer

**Priority / ä¼˜å…ˆçº§**: High

**Estimated Time / é¢„ä¼°æ—¶é—´**: 8 hours

**File / æ–‡ä»¶**: New `tests/performance/`

**Description / æè¿°**:
**ä¸­æ–‡**: è®¾è®¡å’Œå®ç°æ€§èƒ½æµ‹è¯•è®¡åˆ’ï¼ŒåŒ…æ‹¬è´Ÿè½½æµ‹è¯•å’Œå‹åŠ›æµ‹è¯•ã€‚

**English**: Design and implement performance testing plan, including load testing and stress testing.

**Implementation Details / å®ç°ç»†èŠ‚**:

1. **Load Testing Script / è´Ÿè½½æµ‹è¯•è„šæœ¬**:
```python
# tests/performance/load_test.py
from locust import HttpUser, task, between

class PodcastUser(HttpUser):
    wait_time = between(1, 3)

    def on_start(self):
        # Login
        response = self.client.post("/api/v1/auth/login", json={
            "email": "test@example.com",
            "password": "password"
        })
        self.token = response.json()["access_token"]

    @task(3)
    def view_subscriptions(self):
        self.client.get("/api/v1/podcast/subscriptions", headers={
            "Authorization": f"Bearer {self.token}"
        })

    @task(2)
    def search_episodes(self):
        self.client.get("/api/v1/podcast/episodes/search?query=test", headers={
            "Authorization": f"Bearer {self.token}"
        })

    @task(1)
    def get_stats(self):
        self.client.get("/api/v1/podcast/stats", headers={
            "Authorization": f"Bearer {self.token}"
        })
```

2. **Test Scenarios / æµ‹è¯•åœºæ™¯**:
- **Baseline Test**: Single user, measure response times
- **Load Test**: 100 concurrent users, sustained for 10 minutes
- **Stress Test**: Ramp up to 500 concurrent users
- **Spike Test**: Sudden increase from 10 to 200 users

3. **Performance Benchmarks / æ€§èƒ½åŸºå‡†**:
- Podcast list: P95 < 500ms
- Search: P95 < 300ms
- Stats: P95 < 200ms
- Cache hit rate: > 70%

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Design performance test cases (load test, stress test)
- [ ] Implement automated performance tests (Locust/k6)
- [ ] Establish performance baseline and benchmarks
- [ ] Create performance report template
- [ ] Document test results

**Dependencies / ä¾èµ–**: TASK-B-001, TASK-B-002, TASK-B-003, TASK-B-004

**Blocked By / è¢«é˜»å¡**: TASK-B-001, TASK-B-002, TASK-B-003, TASK-B-004

**Notes / å¤‡æ³¨**:
- Use Locust or k6 for load testing
- Run tests in staging environment first

---

### TASK-T-002: Execute Performance Tests / æ‰§è¡Œæ€§èƒ½æµ‹è¯•

**Status / çŠ¶æ€**: ğŸ”´ Pending / å¾…å¤„ç†

**Owner / è´Ÿè´£äºº**: Test Engineer

**Priority / ä¼˜å…ˆçº§**: High

**Estimated Time / é¢„ä¼°æ—¶é—´**: 4 hours

**File / æ–‡ä»¶**: `tests/performance/`

**Description / æè¿°**:
**ä¸­æ–‡**: æ‰§è¡Œæ€§èƒ½æµ‹è¯•ï¼Œæ”¶é›†æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Šã€‚

**English**: Execute performance tests, collect data and generate reports.

**Implementation Details / å®ç°ç»†èŠ‚**:

1. **Test Execution / æµ‹è¯•æ‰§è¡Œ**:
```bash
# Run load test
locust -f tests/performance/load_test.py --host=https://staging.example.com --users 100 --spawn-rate 10 --run-time 10m

# Generate HTML report
locust -f tests/performance/load_test.py --host=https://staging.example.com --users 100 --spawn-rate 10 --run-time 10m --html performance_report.html
```

2. **Metrics Collection / æŒ‡æ ‡æ”¶é›†**:
- Response times (P50, P95, P99)
- Requests per second
- Failure rate
- Cache hit rate
- Database query count

3. **Performance Report / æ€§èƒ½æŠ¥å‘Š**:
```markdown
# Performance Test Report - 2025-01-24

## Test Environment
- Environment: Staging
- Concurrent Users: 100
- Test Duration: 10 minutes

## Results
| Endpoint | P50 (ms) | P95 (ms) | P99 (ms) | Target | Status |
|----------|----------|----------|----------|--------|--------|
| GET /podcast/subscriptions | 150 | 320 | 480 | < 500 | âœ… Pass |
| GET /podcast/episodes/search | 80 | 220 | 350 | < 300 | âœ… Pass |
| GET /podcast/stats | 50 | 120 | 180 | < 200 | âœ… Pass |

## Comparison (Before vs After)
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Avg response time | 2500ms | 150ms | 94% |
| Queries per request | 80 | 3 | 96% |
| Cache hit rate | 0% | 75% | N/A |
```

**Acceptance Criteria / éªŒæ”¶æ ‡å‡†**:
- [ ] Run load tests (simulate 100 concurrent users)
- [ ] Verify all performance targets met
- [ ] Generate performance report with before/after comparison
- [ ] Identify any performance regressions
- [ ] Document test results and findings

**Dependencies / ä¾èµ–**: TASK-T-001, TASK-F-001, TASK-F-002, TASK-F-003, TASK-F-004

**Blocked By / è¢«é˜»å¡**: TASK-T-001, TASK-F-001, TASK-F-002, TASK-F-003, TASK-F-004

**Notes / å¤‡æ³¨**:
- Run tests multiple times for consistency
- Compare against baseline measurements
- Include screenshots of performance graphs

---

## Risk Register / é£é™©ç™»è®°

| Risk / é£é™© | Probability / æ¦‚ç‡ | Impact / å½±å“ | Mitigation / ç¼“è§£æªæ–½ | Status / çŠ¶æ€ |
|------------|------------------|-------------|-------------------|--------------|
| Cache consistency issues / ç¼“å­˜ä¸€è‡´æ€§é—®é¢˜ | Medium | High | Active invalidation + short TTL / ä¸»åŠ¨å¤±æ•ˆ + çŸ­TTL | ğŸ”´ Open |
| Frontend cache showing stale data / å‰ç«¯ç¼“å­˜æ˜¾ç¤ºè¿‡æœŸæ•°æ® | Medium | Medium | Short TTL + active refresh / çŸ­TTL + ä¸»åŠ¨åˆ·æ–° | ğŸ”´ Open |
| Performance optimization breaks features / æ€§èƒ½ä¼˜åŒ–ç ´ååŠŸèƒ½ | Low | High | Comprehensive regression testing / å®Œæ•´å›å½’æµ‹è¯• | ğŸ”´ Open |
| Development overrun / å¼€å‘è¶…æœŸ | Medium | Medium | Phased release, prioritize high-impact items / åˆ†é˜¶æ®µå‘å¸ƒ | ğŸ”´ Open |

---

## Meeting Notes / ä¼šè®®è®°å½•

### Kickoff Meeting / å¯åŠ¨ä¼šè®®

**Date / æ—¥æœŸ**: 2025-01-24

**Attendees / å‚ä¼šè€…**:
- Product Manager
- Backend Developer
- Frontend Developer
- Test Engineer

**Agenda / è®®ç¨‹**:
1. Review requirement document / å®¡æŸ¥éœ€æ±‚æ–‡æ¡£
2. Assign tasks to team members / åˆ†é…ä»»åŠ¡ç»™å›¢é˜Ÿæˆå‘˜
3. Clarify priorities and dependencies / æ˜ç¡®ä¼˜å…ˆçº§å’Œä¾èµ–å…³ç³»
4. Set up communication channels / å»ºç«‹æ²Ÿé€šæ¸ é“

**Action Items / è¡ŒåŠ¨é¡¹**:
- [ ] Backend Developer: Start with TASK-B-001 (highest priority)
- [ ] Frontend Developer: Start with TASK-F-002 (search debounce - user visible)
- [ ] Test Engineer: Prepare test environment for TASK-T-001

**Next Meeting / ä¸‹æ¬¡ä¼šè®®**: 2025-01-27 (Check progress)

---

## Communication Log / æ²Ÿé€šæ—¥å¿—

| Date / æ—¥æœŸ | From / æ¥è‡ª | To / åˆ° | Subject / ä¸»é¢˜ | Summary / æ‘˜è¦ |
|------------|-----------|--------|--------------|--------------|
| 2025-01-24 | PM | All | Requirement created / éœ€æ±‚å·²åˆ›å»º | PRD and task tracking docs created<br>PRDå’Œä»»åŠ¡è·Ÿè¸ªæ–‡æ¡£å·²åˆ›å»º |

---

## Changes / å˜æ›´

| Date / æ—¥æœŸ | Task / ä»»åŠ¡ | Change / å˜æ›´ | Reason / åŸå›  | Approved By / æ‰¹å‡†äºº |
|------------|-----------|--------------|-------------|-------------------|
| - | - | - | - | - |

---

**Last Updated / æœ€åæ›´æ–°**: 2025-01-24

**Next Review / ä¸‹æ¬¡å®¡æŸ¥**: 2025-01-27
